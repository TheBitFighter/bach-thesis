%!TEX spellcheck
%!TEX root = ../bachelor_paper.tex
\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Past approaches}
	\label{ch:prob}
We are not aware of an implementation specifically generating a modified hardware artifact for this purpose. While tracing hardware can be used for this purpose, the scope of data exposed is often limited as metrics focus on software debugging. While hardware independent metrics can be measured this way, hardware bound data like cache misses would have to be simulated based on the trace data, defeating the idea of an in-hardware approach. Additionally, data generated by tracers necessitate post processing as only the trace in binary is exported and no actual performance data is generated. Enabling tracing is also not transparent to the workload running on the artifact tested. Datalynx aims to be indistinguishable in terms of environment behavior and performance by exporting the performance data via a separate bus (see Chapter \ref{ch:arch}). Finally, tracers require (depending on the configuration) much larger data rates than exporting consolidated data like this solution. \\
The basic principals for measuring similarity metrics stay the same to previous attempts without direct hardware support and this thesis pulls heavily from past attempts, especially from the category of feature-aware approaches. This chapter is a short summary of different past attempts at measuring program similarity.

\section{Feature-agnostic approaches}
	\label{sec:prob/agno}
The simplest feature-agnostic approach is to measure program execution time. This method was chosen by Dujmovic and Dujmovic \cite{dujmovicEvolutionEvaluationSPEC1998} in order to gain insight into how different machines perform. Using this information, a set of machines is compared to each other. Two machines are recognized as equal in computational power if they outperform each other by the same ratio in two different, equally weighted benchmarks. This method of measurement ignores all architectural differences of machines and just focuses on results visible from an outside perspective.\\
A very similar methodology was also employed in the much newer paper by Cammerota et al.\ \cite{cammarotaOptimizingProgramPerformance2013}, where in addition to measured values, machine learning was used to fill out unknowns.

Vandierendonck et al.\ \cite{vandierendonckManyBenchmarksStress2004} measures execution cycles, rather than time. This tries to compensate for generational clock speed improvements and focuses on architectural changes only. However, like with \cite{dujmovicEvolutionEvaluationSPEC1998}, a black box approach is taken. A set of machines is measured and arranged as the $n$ dimensions of a performance vector of a benchmark set. These cycle counts are then transformed into principle components by applying \ac{PCA}, which the paper labels \emph{bottlenecks}. Different principal components contribute to the overall performance vector by different amounts, with only a small part of them contributing significantly. These are selected and titled \emph{usage modes}. Four different usage modes are found in the \ac{SPEC} 2000 benchmark suite (\acs{SPEC}int, \acs{SPEC}fp) they examined, which leads them to subset the suite into a set of 9 programs which exercise these usage modes representatively for the entire suite.

These methods have one big advantage as well as one big disadvantage. Evaluating programs like this is simple as virtually no instrumentation is applied to the binary itself. The runtime of the program in question is not modified and no simulation effort is required. Even if the data resolution is low, it is still enough to make certain performance predictions. However, architectural features are completely ignored. Suppose we have a set of machines to determine benchmark similarity but none of these machines feature hardware support for floating point operations. Of the set of examined programs, one happens to be floating point operation heavy and also just so happens to perform similar to one not using floating point operations. When adding a new machine introducing a hardware \ac{FPU}, performance models cannot predict performance anymore as \ac{FPU} operations in the binary were not considered as a characterization criterion. Worse still \cite{vandierendonckManyBenchmarksStress2004} might have marked the benchmark for removal, meaning there is a possibility we would have never found the improvement.

We thus conclude that a pure outside-in perspective using a black box model is not sufficient to determine the similarity of two programs.

\section{Feature-aware approaches}
	\label{sec:prob/aware}
In contrast to the papers presented in Section \ref{sec:prob/agno}, the approaches presented in this section actually consider features exhibited by the programs run. As a hardware artifact consists of several parallel subsystems, workloads are probed in regards to how heavily those subsystems are stressed. This approach naturally requires more probing effort but results in much more fine grained data. Other than the papers in Section \ref{sec:prob/agno}, as long as a certain feature is contained in the signature of a program, two programs cannot be mislabeled similar for that specific feature.

The most prominent publications in this regard are by Phansalkar and Joshi et al.\ \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} as well as Eckhout et al.\ \cite{eeckhoutQuantifyingImpactInput2003}. While their data sets do vary, all of them exhibit a clear intent to measure the impact of certain workloads on different subsystems of a microprocessor. The following describes the main subsystems captured and the metrics to capture them between the different publications.

\subsection{Utilization}
All papers mentioned use \emph{instruction mix} as their measurement of utilization distribution between the different evaluated subsystems. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure percentage of computational instructions, load/store operations, as well as control flow operations (branches), while \cite{eeckhoutQuantifyingImpactInput2003} additionally splits arithmetical operations into integer arithmetic, logical operations, as well as shift- and byte-manipulation operations.

\subsection{Control flow behavior}
Changes in program flow can introduce processing delays like lost cycles if a branching operation cannot be correctly predicted. For this reason, fine grained knowledge about the branching behavior is important to predict program performance as well as measure similarity of workloads. The information is split up into two basic data points: How often do we need to branch and how accurately can we predict whether we need to branch or not. While all three papers measure the mean distance between consecutive branching operations referred to as \emph{basic block size}, they differ in the assumed model of branch prediction. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure percentage of forward pointing branches, percentage of taken branches, and percentage of forward taken branches in regards to taken branches. Meanwhile, \cite{eeckhoutQuantifyingImpactInput2003} employs three different implementations of practical branch predictors, namely a bimodal predictor with 8K entries and 2-bit saturating counters, a gshare predictor also using 2-bit saturating counters and 8ki entries xor-ed with the branching history of the last 12 branches, and a hybrid predictor combining the two and choosing dynamically. Notably, this changes the methodology from purely measuring characteristics of the binary to performance numbers of certain actual implementations.

\subsection{Instruction level parallelism}
\Acf{ILP} describes how many instructions could theoretically be executed concurrently. Again, the papers take a different approach in measuring this data point. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure the register dependency distance in 6 distinct windows of 2, 4, 8, 16, 32, and greater than 32 as an indirect measurement of \ac{ILP}. \cite{eeckhoutQuantifyingImpactInput2003} on the other hand assumes a perfect machine with infinite resources and measures \ac{ILP} this way directly. \Ac{ILP} is useful when designing a superscalar microprocessor, as these benefit from higher inherent parallelism.

\subsection{Cache and data}
While unfortunate control flow breaks can introduce minor stalls, the biggest delay is introduced when the core has to directly access main memory. Because of this, it is especially interesting to investigate data access patterns to gather insight into optimal caching architectures. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} record instruction and data \emph{temporal locality} as well as instruction and data \emph{spacial locality} in windows sizes of 16, 64, 256, and 4096 bytes. \cite{eeckhoutQuantifyingImpactInput2003} simulates five different cache setups with an 8KB and 16KB direct-mapped cache, a 32KB and 64KB two-way set-associative cache as well as an 128KB four-way set-associative cache, each with a block size of 32 bytes, measuring the miss rate of each setup.

Additionally we want to mention the \emph{data stride} metric proposed by Joshi et.al. \cite{joshiDistillingEssenceProprietary2008}, which represents an attempt to preserve and measure patterns shown by the memory access of a program. The difference in addresses between two consecutive memory accesses is measured. This paper is focused on benchmark generation, which we will further elaborate on in section \ref{sec:prob/rel}. This means the data gathered is not averaged but recorded on a per instruction basis. Strides are characterized as 64-bit windows, where an address difference between 0-63 bytes would be considered stride 0, 64-127 bytes would be stride 1, etc. Results of the paper show that most programs from the \acs{SPEC}2000 int and fp suits exhibit a single dominant stride access pattern and more than 90\% show a regular stride behavior.

\subsection{Data space reduction}
All papers exploring feature-aware design exploration mentioned so far perform \acl{PCA} to reduce the value space to be explored \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006,eeckhoutQuantifyingImpactInput2003}. This generates a set of new variables called principal components, where the amount of variance in the data decreases with increasing index. An artificial cut-off is set and all variables of higher index are disregarded. While making clustering simpler and reducing data dimensionality, the output of \ac{PCA} is dependent on the input data set. It is a relative measure, describing the relations between the data points given.  Changing the data set will inevitably change the transformation matrix of input data points to principal components and thus the potentially discarded information. Using \ac{PCA} would require recalculation of the principal components each time a new data point is added and thus inhibit comparability of results. As we are trying to generate a system where absolute data points can be outputted and easily compared, we decided against performing \ac{PCA} on the data generated.

\subsection{Linear model}
Lastly, we want to present a simplified linear model that lies somewhat in between a feature-agnostic and feature-aware approach proposed by Saavedera and Smith \cite{saavedraAnalysisBenchmarkCharacteristics1996}. They propose splitting a program into a set of \acp{AbOp}, where a performance prediction is made by running a machine characterizer on an artifact to gather data about how fast each \ac{AbOp} can be executed. Then, the binary in question is analyzed for the quantity of certain \acp{AbOp} and a simple linear model is applied to calculate predicted performance:

\begin{equation}
\label{eq:theo/simi/hybr/mode}
T_{\mathcal{A},M} = \sum^n_{i = 1}C_{\mathcal{A},i}P_{M,i} = C_\mathcal{A} \cdot P_M
\end{equation}

Where $C_{\mathcal{A}}$ is a vector representing the quantity of each \ac{AbOp} in the binary and $P_{M}$ is the performance vector of the specific machine $M$ (with each dimension being the performance of the respective \ac{AbOp}).

This approach presents a few downsides however. It assumes a linear model for the entire processor as well as little to new influence from cache misses. Additionally, this approach only works with nonoptimized code of the Fortran language. This model would only work in a rather simple processor design without major caching effects, where a linear approximation would be sufficient to characterize the behavior. This model falls short as soon as state based effects like branch prediction or heavy caching comes into play.

\section{Related work}
	\label{sec:prob/rel}

Recent efforts in the field of determining program similarity have largely shifted from pure similarity detection of two workloads onto more applied topics. 

The already mentioned paper by Joshi et al.\ \cite{joshiDistillingEssenceProprietary2008} as well as Ertvelde et al.\ \cite{vanertveldeBenchmarkSynthesisArchitecture2010} try to generate benchmarks by deriving characteristics exhibited by a given potentially commercial workload into a much shorter yet similarly performing workload. The program is executed, measured over a similar set of metrics as employed in this thesis and converted into a labeled control flow graph. Each node in this graph represents a basic block within the original workload and contains information about the instructions executed in the corresponding program section. Edges represent branches and are annotated with their transition probability, which is determined by the branch transition rate in the program. The resulting graph is then minimized by reducing the number of loop iterations as well as the occurrence of basic blocks and finally randomly mapped to C statements mapping to similar but not identical instructions when run through a compiler. They were able to show that this workload performs similar to the input workload, while not revealing information about the input itself. This would enable software manufacturers to publish benchmarks derived from their proprietary workloads for hardware optimization without the fear of reverse engineering.

Breughe et al.\ \cite{breugheSelectingRepresentativeBenchmark2013} evaluates the effect of different inputs on selected benchmarks from the MiBench and SPEC CPU2006 suites and tries to find a minimum number of inputs to create a representative set in terms of \ac{EDP} of a resulting design. They find that randomly selected inputs have a worst case \ac{EDP} of 56\% on average compared to a design point with the minimal \ac{EDP}. They showed through their microarchitectural-independent characterization method (Basic Block Vector or BBV selection as introduced in \cite{sherwoodBasicBlockDistribution2001}) a worst-case \ac{EDP} deficiency of 6.7\% on average with one and 3.7\% with two inputs could be achieved. Min-median-max selection was able to select the design point with minimum \ac{EDP} by using no more than three inputs.

Lastly, several papers have been published about the issue of plagiarism detection in software. In the age of open source it is rather easy to clone a random repository off of pages like GitHub, obfuscate the code and sell it as commercial software. To counter this practice, several methods have been developed to see through possible obfuscation. Dongjin et al.\ \cite{kimMeasuringSimilarityWindows2013} focuses on detecting plagiarism in Microsoft Windows applications using their \emph{birthmark}. This birthmark consists of the frequency of system \acs{API} calls in the binary as well as the frequency of \acs{API} invocations while running the workload. Nair et al.\ \cite{nairFuncGNNGraphNeural2020} represent programs as their control flow graph and use a network to estimate the graph edit distance between two program pairs. In this context, graph edit distance refers to the number of operations needed in order to transform one labeled control flow graph into the other. This value is then used as a measurement of similarity between the two given workloads. Marastoni et al.\ \cite{marastoniDeepLearningApproach2018} apply image recognition techniques; program binaries are converted to fixed size images and fed into a network. They recognized that while possible, shallow neural networks optimized for image recognition might not provide optimal performance for the task of determining program similarity. A deep convolutional neural network structure however was indeed able to see through several layers of code obfuscation. The shallow neural net was able to achieve an accuracy of 0.92 at image recognition after training on the MNIST data set, but was only able to achieve an accuracy of 0.03 at determining plagiarism on a simplified data set. The deep convolutional neural net was able to achieve an accuracy of 0.94 on their simplified set and 0.88 on their full data set.

\section{Summary}
	\label{ch:theo/simi/summ}
A perfect model would be able to predict runtime of an arbitrary workload $\phi$ on a random artifact $\mathcal{A}$ only by knowing its similarity to a different arbitrary workload $\mu$ and its runtime metrics on $\mathcal{A}$. Papers have shown that achieving this feat is possible using different approaches. Both black and whitebox models have shown promise in this regard, however blackbox assumptions do present several downsides like obfuscation of potential changes in hardware design as outlined above.\\
We have also observed that most whitebox models do measure similar if not identical metrics to evaluate program similarity, with some of them even generating synthetic benchmarks from template workloads using the gathered data. For this reason we have chosen a whitebox approach for this thesis further outlined in Chapter \ref{ch:arch}.



% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document}