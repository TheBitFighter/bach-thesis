%!TEX spellcheck
%!TEX root = ../bachelor_paper.tex
\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Problem}
	\label{ch:prob}

According to Wikipedia, a benchmark in computing is
\begin{displayquote}
the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves. \cite{BenchmarkComputing2021}
\end{displayquote}
We will use the word \emph{benchmark} to refer to the latter, a (set of) program(s) constructed or assembled to gauge the performance of an artifact $\mathcal{A}$, in order to gain insight into how said artifact $\mathcal{A}$ might perform in a real world environment compared to a different artifact $\mathcal{B}$. However, even though tempting, using one simple performance score measuring some metric like execution time of a program $\mu$ on artifact $\mathcal{A}$ will, due to the inherent complexity of any modern hardware design, always lead to information loss and thus to an inability of calculating accurate performance numbers for a yet unknown program $\phi$. Solving this issue will be the main focus of this paper. \todo{2.0 und 2.1 ins intro verschieben, rest umstrukturieren mehr ordnung rein}

\section{Benchmark representativness}
	\label{prob/repr}
When selecting hardware platforms for a given application, knowing which platform is actually capable of handling the given task is crucial. For this reason, we employ the aforementioned benchmarks to measure certain characteristics of a particular hardware artifact. However, this requires the assumption of the final workload being similar enough to the benchmarks run. We say the benchmarks have to be representative of the final workload. In the past, there have been several attempts at characterizing the similarity of programs. Most of the papers presented here follow either a feature-agnostic or feature-aware methodology. In order to make this categorization, we use a slightly modified definition introduced by \cite{cammarotaOptimizingProgramPerformance2013} where each property of a program like instruction mix, working set, \ac{ISA}, ... is called a \emph{feature}. A set of features is called a \emph{signature}. An approach is called \emph{feature-agnostic}, iff the features composing the signature used cannot be directly mapped onto easily observable properties of the binary run or the system used. In other words, an approach to similarity looking exclusively at execution time would be feature-agnostic, where an approach investigating properties of the binary executed instead would be considered \emph{feature-aware}.

We create our first simple definition of similarity: Two programs $\mu, \phi$ are similar \todo{add note about names being arbitrary}, if knowledge about the performance of one on a given artifact $\mathcal{A}$ allows us to determine the performance of the other. This definition is rather strict and hard to guarantee given the complexity of modern systems as well as the many different bottlenecks they present. It is important to note that output does not dictate similarity. Two different implementations of the same problem may generate vastly differing binaries stressing very different subsystems of an artifact.

We will lay out a selection of different approaches to program similarity and benchmark representativeness and then present a brief overview over our approach. More details are given in chapter \ref{ch:arch}.

\section{Feature-agnostic approaches}
	\label{prob/angno}
The simplest feature-agnostic approach is to measure program execution time. This method was chosen by Dujmovic et.at. \cite{dujmovicEvolutionEvaluationSPEC1998} in order to gain insight into how different machines perform. Using this information, a set of machines is compared to each other. Two machines are recognized as equal in computational power, if they outperform each other by the same ratio in two different, equally weighted benchmarks. This method of measurement ignores all architectural differences of machines and just focuses on results visible from an outside perspective.\\
A very similar methodology was also employed in the much newer paper by Cammerota et.al. \cite{cammarotaOptimizingProgramPerformance2013}, where in addition to measured values, machine learning was used to fill out unknowns.

Vandierendonck et al. \cite{vandierendonckManyBenchmarksStress} measures execution cycles, rather than time. This hopes to compensate for generational clock speed improvements and focus on architectural changes only. However, like with \cite{dujmovicEvolutionEvaluationSPEC1998}, a black box approach is taken. A set of machines is measured and arranged as the $n$ dimensions of a performance vector of a benchmark set. These cycle counts are then transformed into principle components by applying \ac{PCA}, which the paper labels \emph{bottlenecks}. Different principal components contribute to the overall performance vector by different amounts, with only a small part of them contributing significantly. These are selected and titled \emph{usage modes}. Four different usage modes are found in the \ac{SPEC} benchmark suite they examined, which leads them to subset the suite into a set of 9 programs which exercise these usage modes representatively for the entire suite.

These methods have one big advantage as well as one big disadvantage. Evaluating programs like this is simple as virtually no instrumentation is applied to the binary itself. The runtime of the program in question is not modified and no simulation effort is required. Even if the data resolution is low, it is still enough to make certain performance predictions. However, architectural features are completely ignored. Suppose we have a set of machines to determine benchmark similarity but none of these machines feature hardware support for floating point operations. Of the set of examined programs, one happens to be floating point operation heavy and also just so happens to perform similar to an integer based one. When adding a new machine introducing a hardware \ac{FPU}, performance models cannot predict performance anymore as \ac{FPU} operations in the binary were not considered as a characterization criterion. Worse still \cite{vandierendonckManyBenchmarksStress} might have marked the benchmark for removal, meaning there is a possibility we would have never found the improvement.

We thus conclude that a pure outside-in perspective using a black box model is not sufficient to determine the similarity of two programs, if given the possibility.

\section{Feature-aware approaches}
In contrast to the papers presented in \ref{prob/angno}, we actually consider features exhibited by the programs run. As a hardware artifact consists of several parallel subsystems, workloads are probed in regards to how heavily those subsystems are stressed. This approach naturally requires more probing effort but results in much more fine grained data. Other than the papers in section \ref{prob/angno}, as long as a certain feature is contained in the signature of a program, two programs cannot be mislabeled similar for that specific feature.

The most prominent publications in this regard are by Phansalkar and Joshi et.al. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} as well as Eckhout et.al. \cite{eeckhoutQuantifyingImpactInput}. While their data sets do vary, all of them exhibit a clear intent to measure the impact of certain workloads on different subsystems of a microprocessor. The following describes the main subsystems captured and the metrics to capture them between the different publications.

\subsection{Utilization}
All papers mentioned use \emph{instruction mix} as their measurement of utilization distribution between the different subsystems evaluated. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure percentage of computational instructions, load/store operations, as well as control flow operations (branches), while \cite{eeckhoutQuantifyingImpactInput} additionally splits arithmetical operations into integer arithmetic, logical operations, as well as shift- and byte-manipulation operations.

\subsection{Control flow behavior}
Changes in program flow can introduce processing delays like lost cycles if a branching operation cannot be correctly predicted. For this reason, fine grained knowledge about the branching behavior is important to predict program performance as well as measure similarity of workloads. The information is split up into two basic data points, how often do we need to branch and how accurately can we tell whether we need to branch or not. While all three papers measure the average distance between consecutive branching operations referred to as \emph{basic block size}, they differ in the assumed model of branch prediction. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure percentage of forward pointing branches, percentage of taken branches, and percentage of forward taken branches in regards to taken branches. Meanwhile, \cite{eeckhoutQuantifyingImpactInput} employs three different implementations of practical branch predictors, namely a bimodal predictor with 8K entries and 2-bit saturating counters, a gshare predictor also using 2-bit saturating counters and 8K entries xor-ed with the branching history of the last 12 branches, and a hybrid predictor combining the two and choosing dynamically. Notably, this changes the methodology from purely measuring characteristics of the binary to performance numbers of certain actual implementations.

\subsection{Instruction level parallelism}
\Acf{ILP} describes how many instructions could theoretically be executed concurrently. Again, the papers take a different approach in measuring this data point. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} measure the register dependency distance in 6 distinct windows of 2, 4, 8, 16, 32, and greater than 32 as an indirect measurement of \ac{ILP}. \cite{eeckhoutQuantifyingImpactInput} on the other hand assumes a perfect machine with infinite resources and measures \ac{ILP} this way directly. \Ac{ILP} is useful when designing a superscalar microprocessor, as these benefit from higher inherent parallelism.

\subsection{Cache and data}
While unfortunate control flow breaks can introduce minor stalls, the biggest delay is introduced when the core has to directly access main memory. Because of this, it is especially interesting to investigate data access patterns to gather insight into optimal caching architectures. \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006} record instruction and data \emph{temporal locality} as well as instruction and data \emph{spacial locality} in windows sizes of 16, 64, 256, and 4096 bytes. \cite{eeckhoutQuantifyingImpactInput} simulates five different cache setups with an 8KB and 16KB direct-mapped cache, a 32KB and 64KB two-way set-associative cache as well as an 128KB four-way set-associative cache, each with a block size of 32 bytes, measuring the miss rate of each setup.

Additionally we want to mention the \emph{data stride} metric proposed by Joshi et.al. \cite{joshiDistillingEssenceProprietary2008}, which represents an attempt to preserve and measure patterns shown by the memory access of a program. The difference in addresses between two consecutive memory accesses is measured. This paper is focused on benchmark generation, which we will further elaborate on in section \ref{prob/rel}. This means the data gathered is not averaged but recorded on a per instruction basis. Strides are characterized as 64-bit windows, where an address difference between 0-63 bytes would be considered stride 0, 64-127 bytes would be stride 1, etc. Results of the paper show that most programs exhibit a single dominant stride access pattern and more than 90\% show a regular stride behavior.

\subsection{Data space reduction}
All papers exploring feature-aware design exploration mentioned so far perform \acl{PCA} to reduce the value space to be explored \cite{phansalkarMeasuringProgramSimilarity2005,joshiMeasuringBenchmarkSimilarity2006,eeckhoutQuantifyingImpactInput}. This reduces generates a set of new variables called principal components, where the amount of variance in the data decreases with increasing index. An artificial cut-off is set and all variables of higher index are disregarded. While making clustering simpler, the output of \ac{PCA} is dependent on the input data set. It is a relative measure, describing the relations in the data given. Adding more data will inevitably change the mapping of input data points to principal components. For this reason we will not perform \ac{PCA} on the data generated.

\subsection{Linear model}
Lastly, we want to present a simplified linear model that lies somewhat in between a feature-agnostic and feature-aware approach proposed by Saavedera and Smith \cite{saavedraAnalysisBenchmarkCharacteristics1996}. They propose splitting a program into a set of \acp{AbOp}, where a performance prediction is made by running a machine characterizer on an artifact to gather data about how fast each \ac{AbOp} can be executed. Then, the binary in question is analyzed for the quantity of certain \acp{AbOp} and a simple linear model is applied to calculate predicted performance:

\begin{equation}
\label{eq:theo/simi/hybr/mode}
T_{\mathcal{A},M} = \sum^n_{i = 1}C_{\mathcal{A},i}P_{M,i} = C_\mathcal{A} \cdot P_M
\end{equation}

Where $C_{\mathcal{A}}$ is a vector representing the quantity of each \ac{AbOp} in the binary and $P_{M}$ is the performance vector of the specific machine $M$ (with each dimension being the performance of the respective \ac{AbOp}).

This approach presents a few downsides however. It assumes a linear model for the entire processor as well as little to new influence from cache misses. Additionally, this approach only works with nonoptimized code of the Fortran language. This model would only work in a rather simple processor design without major caching effects, where a linear approximation would be sufficient to characterize the behavior. This model falls short as soon as state based effects like branch prediction or heavy caching comes into play.

\section{Related work}
	\label{prob/rel}

Add stuff about benchmark generation

Add stuff about plagiarism detection



\section{Summary} % TODO
	\label{ch:theo/simi/summ}
A perfect model would be able to predict runtime of a certain workload $\mathcal{A}$ only by knowing its similarity to a different workload $\mathcal{B}$ and the runtime metrics of $\mathcal{B}$. In order to achieve that, we feel it would be important to observe more than just runtime.\\
At the same time, we want to overcome the inherent overhead connected to instrumentation of binary files. Some applications might require production like hardware performance in order to collect data which faithfully represents the use case. We thus extend a RISC-V core implementation to report data from a live execution of an unmodified binary file in order to gather data detailed enough to characterize core utilization.



% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document}