%!TEX spellcheck
%!TEX root = ../bachelor_paper.tex
\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Benchmarks}
    \label{ch:bench}

When looking at possible benchmarks for microarchitectural design evaluation, the number of options is almost as varied as the design options of the architecture implementation itself, and for good reason. Different suits focus on different performance aspects, scoring methods, system requirements and more criteria.\todo{ehhh} While \acs{SPEC} might be one of the most well known suits in general, its requirement for an operating system and focus on high performance render it impractical for an embedded context. At the same time, \acs{SPEC} does not compare performance to power drain as strict power constraints are only vaguely existent in the desktop computing space. In the following we will present the list of benchmarks we selected to compare against our demonstration workload. Naturally, these benchmark suits are designed for an embedded context, but vastly vary in composition, goal, and score evaluation. We feel these benchmarks to be the most commonly used ones when evaluating embedded designs. \todo{neu schreiben}

\section{Coremark}
Coremark is a purely synthetic benchmark aimed at providing a single performance score for as many embedded systems as possible. From the list presented here, Coremark is the only fully synthetic benchmark, which makes the comparison against an actual workload much more interesting. The benchmark consists of three main segments, which are list processing, matrix processing, and state machine processing. List processing iterates through a list of data containing either precomputed values or algorithm invocation instructions, where the list itself is also searched, inversed and sorted. Matrix processing mainly strains the compute engine of a core, performing matrix operations which cannot be computed at compile time. These operations are also meant to test the efficiency of a core's compiler optimizations as well as its handling of tight loop operations and \ac{SIMD} instructions. State machine processing mainly strains the control flow part of a core, using \texttt{if}-statements as the list portion is already implemented using \texttt{switch}-statements \cite{gal-onExploringCoremarkBenchmark2012}.

As already mentioned, its synthetic nature makes Coremark an especially interesting target for comparison against an actual demo workload. Coremark's instruction profile also strongly varies over time \cite{gal-onExploringCoremarkBenchmark2012}, which should be reflected in sectional data as explained in \ref{sec:arch/enlynx}.

\section{MiBench}
MiBench came about it 2001 and was a response to \acs{SPEC} on the desktop side as a standardized benchmark computing suite meant for embedded applications. Its intention was to assemble a portable suite of open source software able to run on as many systems as possible and to be as representative as possible \cite{guthausMiBenchFreeCommercially2001}. In contrast to Coremark, MiBench is a compilation of actual problem solving applications and does not contain purely synthetic elements. MiBench is separated into the categories Automotive and
Industrial Control, Network, Security, Consumer Devices, Office Automation, and Telecommunications. Each category consists of a set of programs representative of applications possibly run on processors of said category at the time of benchmark design as seen in table \ref{tab:bench/mibench/apps}. \cite{guthausMiBenchFreeCommercially2001} goes on to show that MiBench has a higher variation in instruction profiles compared to its desktop counterpart \acs{SPEC}2000. Thus, selecting not only the right sub-suite, but also the right program(s) from the chosen sub-suite(s) poses an interesting challenge to solve. However, the benchmark suite also assumes a host operating system for most of it's programs \cite{pallisterBEEBSOpenBenchmarks2013}. This is a rather steep constraint and applications with that requirement will not be tested on our platform. \todo{check this in benchmark script}

\begin{table}
    \centering
    \begin{tabular}{llllll}
        \textbf{Auto./Industrial}   & \textbf{Consumer} & \textbf{Office}   & \textbf{Network}  & \textbf{Security}     & \textbf{Telecom.} \\
        \hline\\[-0.9em]
        basicmath           & jpeg          & ghostscript   & dijkstra  & blowfish enc. & CRC32     \\
        bitcount            & lame          & ispell        & patricia  & blowfish dec. & FFT       \\
        qsort               & mad           & rsynth        & (CRC32)   & pgp sign      & IFFT      \\
        susan (edges)       & tiff2bw       & sphinx        & (sha)     & pgp verify    & ADPCM enc.\\
        susan (corners)     & tiff2rgba     & stringsearch  & (blowfish)& rijndael enc. & ADPCM dec.\\
        susan (smoothing)   & tiffdither    &               &           & rijndael dec. & GSM enc.  \\
                            & tiffmedian    &               &           & sha           &           \\
                            & typeset       &               &           &               &           \\
        \hline
    \end{tabular}
    \caption{Applications contained in the different sub-suites of MiBench \cite{guthausMiBenchFreeCommercially2001}}
    \label{tab:bench/mibench/apps}
\end{table}

\section{BEEBS}
BEEBS' main focus is the exploration of energy consumption. The corresponding whitepaper was published in 2013 and is designed for embedded applications. BEEBS is a conglomerate of workloads cherry picked from already existing benchmark suites with a main focus on providing applications big enough to be representative while setting as little requirements for the host platform as possible (storage requirements, file system, ...). They group the applications contained by four different categories corresponding to strain exerted on different subsystems of a core under test. Each of the categories, integer operations, floating point operations, memory access intensity, and branching frequency, are marked with either low, medium, or high \cite{pallisterBEEBSOpenBenchmarks2013}. The benchmarks were mostly sourced from the MiBench suite \cite{guthausMiBenchFreeCommercially2001} with 3 applications being derived from the WCET suite \cite{gustafssonMalardalenWCETBenchmarks2010} and one from DSPStone \cite{zivojnovicDSPstoneDSPorientedBenchmarking1994}, as seen in table \ref{tab:bench/beebs/apps}. The table also shows their categorization according to the grouping criteria laid out by \cite{pallisterBEEBSOpenBenchmarks2013}. The resulting set was tested on different architectures representing different tiers of processors. As the main metric to be observed is power consumption, a memory access on an implementation featuring a cache uses a different amount of energy than the same access on an implementation without one. The applications were validated on a Cortex-M0 processor, a simple single core implementation, an XMOS L1, an event driven platform with eight hardware threads, and Epiphany, a super scalar 16-core processor \cite{pallisterBEEBSOpenBenchmarks2013}. The benchmarks were validated by generating instruction traces in order to ensure a large enough spread in operation distribution.

BEEBS' most interesting characteristic in our context is the difference in pipelines for multiplication in RI5CY as explained in section \ref{sec:plat/pipe}. 

\begin{table}
    \centering
    \begin{tabular}{lllllll}
    \textbf{Name}   & \textbf{Source}   & \textbf{B} & \textbf{M} & \textbf{I} & \textbf{FP}    & \textbf{Category} \\
    \hline\\[-0.9em]
    Blowfish        & MiBench   & L & M & H & L & Security \\
    CRC32           & MiBench   & M & L & H & L & Telecom. \\
    Cubic root solver & MiBench & L & M & H & L & Automotive \\
    Dijkstra        & MiBench   & M & L & H & L & Network  \\
    FDCT            & WCET      & H & H & L & H & Consumer \\
    Float Matmult   & WCET      & M & H & M & M & Automotive, consumer \\
    Integer Matmult & WCET      & M & M & H & L & Automotive \\
    Rijndael        & MiBench   & H & L & M & L & Security \\
    SHA             & MiBench   & H & M & M & L & Network, security \\
    2D FIR          & DSPstone  & H & M & L & H & Automotive, consumer \\
    \hline
    \end{tabular}
    \caption{Applications contained in BEEBS, their origins, and their categorization \cite{pallisterBEEBSOpenBenchmarks2013}}
    \label{tab:bench/beebs/apps}
\end{table}

\section{Embench}
\todo{add this}
% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document} 
