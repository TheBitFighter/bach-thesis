\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Introduction}
    \label{ch:intro}

Just having bought a new processor or computer, first thing many people do is take their new toy out for a spin. There is a universal allure to putting your new precious to the test like testing the limits of a new car or playing the first riff on a guitar. But in microprocessors, knowing just how fast it can go is an integral part of the development process of the core, its immediate surrounding systems, as well as any product using said microprocessor or core. How can we be sure the controller of an actuator doesn't get overwhelmed and the plane loses control?

The process of evaluating just how fast the \ac{MPU} can run is called a benchmark. Over the decades, creating these benchmarks has gained more and more scientific attention, evolving from attempts to hit as many common workloads as possible to analyzing similarity between the different programs and input sets employed by a benchmark suit. In this thesis we will start by looking at several different benchmarks relevant mostly to \ac{ISA}s supporting Linux-like operating systems, as well as Coremark\cite{gal-onExploringCoremarkBenchmark2012}, a platform agnostic benchmark suitable for smaller cores, not limited to Linux-like \ac{OS} supporting platforms. Second, we will look at different attempts to evaluate similarity of programs in a single benchmark suit and the ``quality'' of a benchmark suit in general. With the insight gathered we will start constructing our own architecture meant to integrate with a modified RISC-V core with the goal of being able to measure just how similar two different programs actually are.

\section{Motivation}
Since the start of the new century, evaluating the quality of benchmarks has received rising scientific attention \cite{eeckhoutDesigningComputerArchitecture2003,dujmovicEvolutionEvaluationSPEC1998,vandierendonckManyBenchmarksStress,phansalkarMeasuringProgramSimilarity2005,eeckhoutQuantifyingImpactInput}, especially for platforms supporting Linux-like operating systems. These papers mainly focus on reducing the amount of actual benchmarking programs in a suit to speed up the process of evaluating a single system while retaining the quality of the result. 

However, there are two main issues. First, to our knowledge, there exists little research for these challenges outside the Linux-like space (e.g., \ac{MPU}s without support for privileged instructions and other systems required by a Linux-like \ac{OS}). And second, to our knowledge, there exists little research on similarity of programs actually used in the field and benchmarks proposed for these micro processors.

When engineering a new core implementing a certain \ac{ISA}, or building a product around such a core, it is important to have tools to evaluate a performance metric which is able to predict the real world performance in actual deployment. The lack of such tools can lead to a too powerful core being chosen in the best case, which adds cost and complexity, or a too weak core which, in the worst case, can lead to failure of the system. It is thus important to know, if the tools we have now actually hold up in comparison to real world use cases. We take the first step in this exploration by providing the tools necessary for measuring this information.

When working with \ac{MPU}s, only so much information can be extracted by simulation, as it is extremely slow \cite{eeckhoutDesigningComputerArchitecture2003,kaoHardwareApproachRealTime2007} and some inputs cannot be simulated altogether, especially in real-time applications \cite{kaoHardwareApproachRealTime2007}. It is possible to augment an existing binary with instructions meant to monitor performance counters \cite{eeckhoutQuantifyingImpactInput}, this however will inevitably alter the performance of the core on which the modified binary is run on as well as alter the frequency of hardware based events like cache misses if applicable. Obviously, our presented minimal FPGA-based implementation is far from an actual high performance core; such an implementation is left for further works. But it is easy to see that a hardware based solution watching an otherwise mostly vanilla core is as close to real-world as possible.

Finally, we chose the RISC-V ISA \cite{RISCVTechnicalSpecification} as it is open source and free to use. There are already many core designs freely available\footnote{\url{https://riscv.org/exchange/cores-socs/}}. Despite its rather recent inception in 2010, it has already gained a substantial following due to its technical elegance as well as its open nature. Our modified core is based on PULPissimo \cite{PulpplatformPulpissimo2021}, a cooperation of the University of Bologna and the ETH Z\"urich.

\section{Contributions}
\begin{itemize}
\item We provide a set of architecture/implementation dependent and architecture/implementation independent set of metrics for measuring the similarity of two different programs.
\item We provide the theoretical groundwork for a hardware based toolset for measuring these parameters.
\item We provide a minimalistic prototype implementation of the described instrumentation platform.
\end{itemize}

% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document}