%!TEX spellcheck
%!TEX root = ../bachelor_paper.tex
\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Introduction}
    \label{ch:intro}

According to Wikipedia, a benchmark in computing is
\begin{displayquote}
the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves. \cite{BenchmarkComputing2021}
\end{displayquote}
We will use the word \emph{benchmark} to refer to the latter, a (set of) program(s) constructed or assembled to gauge the performance of an artifact $\mathcal{A}$, in order to gain insight into how said artifact $\mathcal{A}$ might perform in a real world environment compared to a different artifact $\mathcal{B}$. However, even though tempting, using one simple performance score measuring some metric like execution time of an arbitrary program $\mu$ on artifact $\mathcal{A}$ will, due to the inherent complexity of any modern hardware design, always lead to information loss and thus to an inability of calculating accurate performance numbers for a yet unknown different arbitrary program $\phi$. Providing the first steps to solving this issue will be the main focus of this thesis.

\section{Motivation}
Since the start of the new century, evaluating the quality of benchmarks has received rising scientific attention \cite{eeckhoutDesigningComputerArchitecture2003,dujmovicEvolutionEvaluationSPEC1998,vandierendonckManyBenchmarksStress,phansalkarMeasuringProgramSimilarity2005,eeckhoutQuantifyingImpactInput}, especially for platforms supporting Linux-like operating systems. These papers mainly focus on reducing the amount of actual benchmarking programs in a suit, in order to speed up the process of evaluating a single system while retaining almost all information contained in the result. 

However, to our knowledge, there exists little research on similarity of programs actually used in the field and benchmarks proposed for these micro processors.

When engineering a new core implementing a certain \ac{ISA}, or building a product around such a core, it is imperative to be able to predict the real world performance in actual deployment. The lack of such knowledge can lead to a too powerful core being chosen in the best case, which adds cost and complexity, or a too weak core which, in the worst case, can lead to system failure. Knowledge about the adequacy of the chosen tools is thus essential. We take the first step in this exploration by providing the underlying tools necessary for measuring this information.

When working with \ac{MCU}s, only so much information can be extracted by simulation, as doing so is extremely slow \cite{eeckhoutDesigningComputerArchitecture2003,kaoHardwareApproachRealTime2007} and some inputs cannot be easily simulated altogether, especially in real-time applications \cite{kaoHardwareApproachRealTime2007}. It is possible to augment an existing binary with instructions meant to monitor performance counters \cite{eeckhoutQuantifyingImpactInput}, this however will inevitably alter the performance of the core on which the modified binary is run on as well as alter the frequency of hardware based events like cache misses if applicable. Obviously, our presented minimal FPGA-based implementation is far from an actual high performance core; such an implementation is left for further works. But it is easy to see that a hardware based solution monitoring an otherwise mostly untouched core is as close to real-world as possible.

Finally, we chose the RISC-V ISA \cite{RISCVTechnicalSpecification} as it is open source and free to use. There are already many core designs available\footnote{\url{https://riscv.org/exchange/cores-socs/}} despite its rather recent inception in 2010, and it has already gained substantial recognition especially in academics due to its open nature. Our modified core is based on PULPissimo \cite{GitHubPulpplatformPulpissimo2021}, a cooperation of the University of Bologna and the ETH Z\"urich.

\section{Benchmark representativness}
    \label{sec:prob/repr}
As mentioned, when selecting hardware platforms for a given application, knowing which platform is actually capable of handling the given task is crucial. For this reason, we employ the aforementioned benchmarks to measure certain characteristics of a given hardware artifact. This however requires the assumption of the final workload being similar enough to the benchmarks run. We say the benchmarks need to be \emph{representative} of the final workload. There have been several attempts at characterizing the similarity of programs in the past. Most of the approaches presented here follow either a feature-agnostic or feature-aware methodology. In order to make this categorization, we use a slightly modified definition introduced by \cite{cammarotaOptimizingProgramPerformance2013} where each property of a program like instruction mix, working set, \ac{ISA}, ... is called a \emph{feature}. A set of features is called a \emph{signature}. An approach is called \emph{feature-agnostic}, iff the features composing the signature used cannot be directly mapped onto easily observable properties of the binary run or the system used. In other words, an approach to similarity looking exclusively at execution time would be feature-agnostic, where an approach investigating properties of the binary executed instead would be considered \emph{feature-aware}. We decided to take a feature-aware approach by selecting a signature representative of the underlying architecture and start by implementing a logging system able to gather this data. The result should be able to inform us about the similarity of two given workloads.

We thus define our notion of similarity: Two arbitrary programs $\mu, \phi$ are similar, iff knowledge about the performance of one on a given artifact $\mathcal{A}$ allows us to determine the performance of the other within a factor $\epsilon$. The smaller $\epsilon$, the better our measurement of similarity. \\
This definition is rather strict and hard to guarantee given the complexity of modern systems as well as the many different bottlenecks they present. It is important to note that output \emph{does not} dictate similarity. Two different implementations of the same problem may generate vastly differing binaries stressing very different subsystems of an artifact.


\section{Contributions}
\begin{itemize}
\item We provide a hardware framework capable of logging in-flight and total workload telemetry data based on a defined feature signature.
\item We demonstrate the logging capabilities of this framework using industry standard benchmarks.
\end{itemize}

% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document}