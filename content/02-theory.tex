\documentclass[../bachelor_paper.tex]{subfiles}
\graphicspath{{\subfix{images/}}}
\begin{document}

\chapter{Theory}
    \label{ch:theory}

According to Wikipedia, a benchmark in computing is \emph{the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves.} \cite{BenchmarkComputing2021} \\
We will use the work \emph{benchmark} to refer to the latter, a (set of) program(s) constructed or assembled to gage the performance of an artifact, in order to gain insight on how said artifact might perform in a real world application compared to other artifacts. It is important to note however, that interpolating hard performance numbers of a target program from the sole run of a benchmark is almost impossible since different programs have different characteristics. This problem will be the main focus of this paper.\\

\section{Benchmarks}
	\label{ch:theory/benchmarks}
We will start this section by mentioning the two in scientific circles ost well known benchmark suits, the \emph{SPEC suit} and \emph{Coremark}.

\subsection{SPEC suite}
	\label{ch:theory/benchmarks/spec}
The \ac{SPEC} is a non-profit corporation, founded in 1988 by Apollo, Hewlett Packard, MIPS and Sun Microsystems. \cite{dixitSPECBenchmarks1991} The idea was to provide uniform tools to evaluate performance of an artifact in a way where it could be compared to a architecturally different artifact. This however posed one of the greatest questions: What even is performance? And \todo{remove the joke} if yes, how does one measure it?\\
The biggest problem at hand was the fundamental difference between the systems, \ac{SPEC}\footnote{\ac{SPEC} is used for the institution and their benchmarking suite interchangeably} aims to provide. According to the official website of \ac{SPEC} CPU\rsym 2017, the current iteration of the CPU benchmark offered by \ac{SPEC}, toolsets for ARM, Power ISA, SPARC, and x86 are provided. It is possible to easily port the benchmarks for other \ac{ISA}s as well, should one need it. The benchmarks were specifically selected to be easily portable between different platforms. Modifications were added to make the code as platform agnostic, and the codepath as uniform as possible.

This reveals the second issue \ac{SPEC} has: The selection of benchmarks is more democratic than scientific. When a new iteration of \ac{SPEC} CPU\rsym is created, \ac{SPEC} puts out a call for programs representing real life workloads and meeting their portability criteria. Members of the board vote for the inclusion of a particular workload. \cite{henningSPECCPU2000Measuring2000} This means the representation of workloads is somewhat balanced, as no architecture shall be favored; however vendor interest is hardly a scientific criterion. 

The \ac{SPEC} suit stresses the toolchain as a whole as the programs are provided as source code. Different compiler settings thus may lead to vastly different results on a single platform. \ac{SPEC} counters this by adding a full system report to the result of a benchmark run and encrypting them. \cite{bucekSPECCPU2017NextGeneration2018} Still, different toolchains may react differently to certain code patters. One of the criteria for inclusion is the predictability of the codepath, but while somewhat predictable, they are still not identical. 

And finally, \ac{SPEC} CPU\rsym requires the use of a Unix like \ac{OS} or Windows\footnote{For further information see \cite{SystemRequirementsCPU}} and at least 1Gbyte of \ac{RAM} for \ac{SPEC}rate per copy when compiled in 32 bit and up to 16Gbyte for \ac{SPEC}speed. It is trivial to see how those two are knockout criteria for an \ac{MPU} focused core. Nevertheless, \ac{SPEC} is easily the most well studied benchmark suit out there. It is a well put together suit of programs, meant to test the limit of high performance machines. It stresses the system as a whole in a rather extensive set of use cases. We will come back to \ac{SPEC} in section \todo{blah}.

\subsection{Coremark}
	\label{ch:theory/benchmarks/coremark}
While the idea of Coremark is similar to \ac{SPEC}, they approach the problem from the opposite angle. To our knowledge, not a lot of research papers have been published on Coremark, most of the information given here is from the whitepaper \cite{gal-onExploringCoremarkBenchmark2012}.

Similar to \ac{SPEC}, Coremark tries to come up with a single performance number to characterize a given core. Where \ac{SPEC} consists of real world programs adapted to work as a benchmarking workload, Coremark consists of an artificially constructed workload engineered to stress different subsystems separately to the best of its abilities. The workload still stays the same for different artifacts so while a specific workload is still handled in cache on one artifact, it might be already stressing the memory system on another.

While Coremark takes steps to ensure portability of code between different artifacts like not using malloc, \ac{ISA} specific tools like \ac{SIMD} instructions and \ac{MAC unit}s are still used as they ultimately contribute to the overall performance of an artifact. Just like \ac{SPEC}, the compiler still has significant influence over how well a given artifact might perform. E.g. it can be configured to generate a smaller executable to fit on systems with less storage, which drops the performance roughly by 19\% \cite{gal-onExploringCoremarkBenchmark2012}. While this might suggest a large file size, \textit{smaller} may be misleading. By default, Coremark requires a total space of roughly 2 Kbytes compared to the minimum memory of 1 Gbyte for \ac{SPEC}rate in 32 Bit mode and a total install size of 250 Gbyte (or 1.2 Gbyte for \ac{SPEC}rate per copy). \cite{SystemRequirementsCPU}

We will use Coremark as a target to analyze, since it is the most used benchmark for the kind of platforms this paper is targeting.

\section{Similarity of programs}
	\label{ch:theory/similarity}
When looking at research done on the similarity of programs and how to measure it, three main approaches can be characterized. This distinction is not a clear one and very rough since most papers employ very different methods on the classification of \textit{difference}. 

First, we feel the necessity to clarify that two programs being different does not mean they necessarily \textit{output} something different. Two different programs calculating the same result can generate vastly different binaries, depending on which algorithm was used. At the same time, algorithms used to calculate two different problems can produce similar binaries. So we define two programs $a,b$ to be \textit{similar}, if knowledge of the performance of $a$ on any artifact $\mathcal{A}$ allows inference of the performance of $b$ on $\mathcal{A}$ within margin of error. This is a rather strict definition and hard to actually achieve in the real world. It is however what we try to aim for with benchmarks, since their purpose is to gain knowledge about the performance of a program which, for some reason, cannot be run instead.

The three main approaches to measuring the similarity of programs seen are what we refer to as \textit{hardware}, \textit{software}, and \textit{hybrid approach}. Note that these terms are rather artificial, since no paper found actually employs modified hardware. It is much rather the kind of data measured which defines the "line" between them. This distinction becomes even harder to make as the border of what can be analyzed from software beforehand and what needs to be simulated in hardware dilutes even further with hybrid approaches.

\subsection{Hardware approach}
	\label{ch:theory/similarity/hardware}
We refer to these kinds of papers as \textit{hardware approach} since the data gathered mostly looks at what the hardware is doing. The two notable papers in this category are by Dujmovic, and Dujmovic \cite{dujmovicEvolutionEvaluationSPEC1998} and Vandierendonck and Bosschere \cite{vandierendonckManyBenchmarksStress}. The main idea between both of them is to measure how the hardware reacts to certain workloads. Both measure different forms of benchmark duration. 

Dujmovic et.at. \cite{dujmovicEvolutionEvaluationSPEC1998} only measures the benchmark duration in time to gain insight on how different machines perform. Using this information, a set of machines is compared to each other. Two machines are seen as equal in computational power, if they outperform each other by the same ratio in two different benchmarks with equal weighting. This measuring method ignores all architectural differences of machines and just focuses on what can be measured from an outside perspective.\\
However, while the math makes sense in theory, it ignores the ever changing landscape of computing. It draws conclusions from a ``representative'' set of machines and proposes removal of benchmarks which perform similar to others. Why is this a problem? Suppose non of the machines have a hardware \ac{FPU}. Let there be a benchmark suit containing a floating point heavy benchmark. If this benchmark just so happens to perform very similar to some integer based one, it might be marked for removal. Now we construct a new machine which contains a hardware \ac{FPU}. Naturally, this machine will outperform all the others in the now removed benchmark, as floating point operations do not have to be emulated anymore. It is trivial to see the issue.

Vandierendonck et al. \cite{vandierendonckManyBenchmarksStress} looks at execution cycles, rather than time. This hopes to compensate for generational clock speed improvements and focus on architectural changes. However, like with \cite{dujmovicEvolutionEvaluationSPEC1998}, a black box approach is taken. A set of machines is tested and measured. The execution clock cycles are transformed into an arbitrary set of ``bottlenecks'' which they assume to exist within the tested machines using \ac{PCA}. The components contributing more variance than a single system are selected to be the \textit{usage modes} of a benchmark set. This leads to them finding four different usage modes within the \ac{SPEC} benchmark set and concluding that about 9 benchmarks would be representative of the whole set.\\
While abstracting the question at hand, they suffer from the exact same issue as \cite{dujmovicEvolutionEvaluationSPEC1998}. Furthermore, we know that there exists a bottleneck, but we don't know what it is. While this is helpful for comparing benchmarks for architecturally similar artifacts, it again falls apart as soon as the environment changes.

Both these papers lead to the conclusion that looking from the outside in without any knowledge of hardware features or software composition is not sufficient for determining similarity of software.

\subsection{Software approach}
	\label{ch:theory/similarity/software}
We sort papers into the \emph{software approach} category, if they (almost)exclusively look at data gathered from analyzing program binaries. The two main publications to mention here are by Phansalkar et al. \cite{phansalkarMeasuringProgramSimilarity2005} and Eeckhout et al. \cite{eeckhoutDesigningComputerArchitecture2003}

\subsection{Hybrid approach}
	\label{ch:theory/similarity/hybrid}


% Important commands: \todo{}, \ac{} for acronyms

% Render bibliograhy and acronyms if rendered standalone
\isstandalone
\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\subfile{abbreviations.tex}
\fi

\end{document}