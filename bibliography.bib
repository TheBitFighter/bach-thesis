
@misc{ARMEmbeddedTrace,
  title = {{{ARM Embedded Trace Macrocell Architecture Specification ETMv4}}.0 to {{ETMv4}}.2},
  langid = {english},
  file = {/home/fabian/Zotero/storage/66HYSIHL/ARM Embedded Trace Macrocell Architecture Specific.pdf}
}

@inproceedings{barrosoMemorySystemCharacterization1998,
  title = {Memory System Characterization of Commercial Workloads},
  booktitle = {Proceedings. 25th {{Annual International Symposium}} on {{Computer Architecture}} ({{Cat}}. {{No}}.{{98CB36235}})},
  author = {Barroso, L. A. and Gharachorloo, K. and Bugnion, E.},
  year = {1998},
  month = jul,
  pages = {3--14},
  issn = {1063-6897},
  doi = {10.1109/ISCA.1998.694758},
  abstract = {This paper presents a detailed performance study of three important classes of commercial workloads: online transaction processing (OLTP), decision support systems (DSS), and Web index search. We use the Oracle commercial database engine for our OLTP and DSS workloads, and the AltaVista search engine for our Web index search workload. This study characterizes the memory system behavior of these workloads through a large number of architectural experiments on Alpha multiprocessors augmented with full system simulations to determine the impact of architectural trends. We also identify a set of simplifications that make these workloads more amenable to monitoring and simulation without affecting representative memory system behavior. We observe that systems optimized for OLTP versus DSS and index search workloads may lead to diverging designs, specifically in the size and speed requirements for off-chip caches.},
  keywords = {Alpha multiprocessors,AltaVista search engine,Application software,commercial workloads,database management systems,Databases,decision support systems,Decision support systems,Delay,Design engineering,Design optimization,Engines,full system simulations,Laboratories,memory system behavior,memory system characterization,monitoring,online transaction processing,Oracle commercial database engine,outdated,parallel architectures,performance study,representative memory system behavior,shared memory systems,simulation,Technological innovation,transaction processing,Web index search,Web server},
  file = {/home/fabian/Zotero/storage/SBZYLG93/Barroso et al_1998_Memory system characterization of commercial workloads.pdf;/home/fabian/Zotero/storage/V8RWET4E/694758.html}
}

@article{BenchmarkComputing2021,
  title = {Benchmark (Computing)},
  year = {2021},
  month = mar,
  journal = {Wikipedia},
  abstract = {In computing, a benchmark is the act of running a computer program, a set of programs, or other operations, in order to assess the relative performance of an object, normally by running a number of standard tests and trials against it. The term benchmark is also commonly utilized for the purposes of elaborately designed benchmarking programs themselves. Benchmarking is usually associated with assessing performance characteristics of computer hardware, for example, the floating point operation performance of a CPU, but there are circumstances when the technique is also applicable to software. Software benchmarks are, for example, run against compilers or database management systems (DBMS). Benchmarks provide a method of comparing the performance of various subsystems across different chip/system architectures. Test suites are a type of system intended to assess the correctness of software.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1011181898},
  file = {/home/fabian/Zotero/storage/U94RYPEK/index.html}
}

@article{breugheSelectingRepresentativeBenchmark2013,
  title = {Selecting Representative Benchmark Inputs for Exploring Microprocessor Design Spaces},
  author = {Breughe, Maximilien B. and Eeckhout, Lieven},
  year = {2013},
  month = dec,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {10},
  number = {4},
  pages = {37:1--37:24},
  issn = {1544-3566},
  doi = {10.1145/2541228.2555294},
  abstract = {The design process of a microprocessor requires representative workloads to steer the search process toward an optimum design point for the target application domain. However, considering a broad set of workloads to cover the large space of potential workloads is infeasible given how time-consuming design space exploration typically is. Hence, it is crucial to select a small yet representative set of workloads, which leads to a shorter design cycle while yielding a (near) optimal design. Prior work has mostly looked into selecting representative benchmarks; however, limited attention was given to the selection of benchmark inputs and how this affects workload representativeness during design space exploration. Using a set of 1,000 inputs for a number of embedded benchmarks and a design space with around 1,700 design points, we find that selecting a single or three random input(s) per benchmark potentially (in a worst-case scenario) leads to a suboptimal design that is 56\% and 33\% off, on average, relative to the optimal design in our design space in terms of Energy-Delay Product (EDP). We then propose and evaluate a number of methods for selecting representative inputs and show that we can find the optimum design point with as few as three inputs.},
  keywords = {input selection,Processor design space exploration,workload selection},
  file = {/home/fabian/Zotero/storage/SG8QBPJU/Breughe_Eeckhout_2013_Selecting representative benchmark inputs for exploring microprocessor design.pdf}
}

@inproceedings{bucekSPECCPU2017NextGeneration2018,
  title = {{{SPEC CPU2017}}: {{Next-Generation Compute Benchmark}}},
  shorttitle = {{{SPEC CPU2017}}},
  booktitle = {Companion of the 2018 {{ACM}}/{{SPEC International Conference}} on {{Performance Engineering}}},
  author = {Bucek, James and Lange, Klaus-Dieter and {v. Kistowski}, J{\'o}akim},
  year = {2018},
  month = apr,
  pages = {41--42},
  publisher = {{ACM}},
  address = {{Berlin Germany}},
  doi = {10.1145/3185768.3185771},
  abstract = {Description of the new features of the SPEC CPU2017 industry standard benchmark and its metric calculations.},
  isbn = {978-1-4503-5629-9},
  langid = {english},
  file = {/home/fabian/Zotero/storage/H44QRAUE/Bucek et al. - 2018 - SPEC CPU2017 Next-Generation Compute Benchmark.pdf}
}

@inproceedings{cammarotaOptimizingProgramPerformance2013,
  title = {Optimizing {{Program Performance}} via {{Similarity}}, {{Using}} a {{Feature-Agnostic Approach}}},
  booktitle = {Advanced {{Parallel Processing Technologies}}},
  author = {Cammarota, Rosario and Beni, Laleh Aghababaie and Nicolau, Alexandru and Veidenbaum, Alexander V.},
  editor = {Wu, Chenggang and Cohen, Albert},
  year = {2013},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {199--213},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-45293-2_15},
  abstract = {This work proposes a new technique for performance evaluation to predict performance of parallel programs across diverse and complex systems. In this work the term system is comprehensive of the hardware organization, the development and execution environment.The proposed technique considers the collection of completion times for some pairs (program, system) and constructs an empirical model that learns to predict performance of unknown pairs (program, system). This approach is feature-agnostic because it does not involve previous knowledge of program and/or system characteristics (features) to predict performance.Experimental results conducted with a large number of serial and parallel benchmark suites, including SPEC CPU2006, SPEC OMP2012, and systems show that the proposed technique is equally applicable to be employed in several compelling performance evaluation studies, including characterization, comparison and tuning of hardware configurations, compilers, run-time environments or any combination thereof.},
  isbn = {978-3-642-45293-2},
  langid = {english},
  keywords = {Cluster Analysis,Empirical Performance Modeling,Feature-agnostic,Program Characterization,Program Optimization},
  file = {/home/fabian/Zotero/storage/JJS866E2/Cammarota et al_2013_Optimizing Program Performance via Similarity, Using a Feature-Agnostic Approach.pdf}
}

@inproceedings{cheCharacterizationRodiniaBenchmark2010,
  title = {A Characterization of the {{Rodinia}} Benchmark Suite with Comparison to Contemporary {{CMP}} Workloads},
  booktitle = {{{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}}'10)},
  author = {Che, Shuai and Sheaffer, Jeremy W. and Boyer, Michael and Szafaryn, Lukasz G. and Wang, Liang and Skadron, Kevin},
  year = {2010},
  month = dec,
  pages = {1--11},
  doi = {10.1109/IISWC.2010.5650274},
  abstract = {The recently released Rodinia benchmark suite enables users to evaluate heterogeneous systems including both accelerators, such as GPUs, and multicore CPUs. As Rodinia sees higher levels of acceptance, it becomes important that researchers understand this new set of benchmarks, especially in how they differ from previous work. In this paper, we present recent extensions to Rodinia and conduct a detailed characterization of the Rodinia benchmarks (including performance results on an NVIDIA GeForce GTX480, the first product released based on the Fermi architecture). We also compare and contrast Rodinia with Parsec to gain insights into the similarities and differences of the two benchmark collections; we apply principal component analysis to analyze the application space coverage of the two suites. Our analysis shows that many of the workloads in Rodinia and Parsec are complementary, capturing different aspects of certain performance metrics.},
  keywords = {Benchmark testing,Computational fluid dynamics,Computer architecture,Graphics processing unit,Heart,Instruction sets,Kernel},
  file = {/home/fabian/Zotero/storage/J3MDX9BP/Che et al_2010_A characterization of the Rodinia benchmark suite with comparison to.pdf;/home/fabian/Zotero/storage/XJ4Z3L8J/5650274.html}
}

@inproceedings{deckerRapidlyAdjustableNonintrusive2017,
  title = {Rapidly {{Adjustable Non-intrusive Online Monitoring}} for {{Multi-core Systems}}},
  booktitle = {Formal {{Methods}}: {{Foundations}} and {{Applications}}},
  author = {Decker, Normann and Gottschling, Philip and Hochberger, Christian and Leucker, Martin and Scheffel, Torben and Schmitz, Malte and Weiss, Alexander},
  editor = {Cavalheiro, Simone and Fiadeiro, Jos{\'e}},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {179--196},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-70848-5_12},
  abstract = {This paper presents an approach for rapidly adjustable embedded trace online monitoring of multi-core systems, called RETOM. Today, most commercial multi-core SoCs provide accurate runtime information through an embedded trace unit without affecting program execution. Available debugging solutions can use it to reconstruct the run offline, but usually for up to a few seconds only. RETOM employs a novel online reconstruction technique that makes the program run available outside the SoC and allows for evaluating a specification formulated in the stream-based specification language TeSSLa in real time. The necessary computing performance is provided by an FPGA-based event processing system. In contrast to other hardware-based runtime verification techniques, changing the specification requires no circuit synthesis and thus seconds rather than minutes or hours. Therefore, iterated testing and property adjustment during development and debugging becomes feasible while preserving the option of arbitrarily extending observation time, which may be necessary to detect rarely occurring errors. Experiments show the feasibility of the approach.},
  isbn = {978-3-319-70848-5},
  langid = {english},
  file = {/home/fabian/Zotero/storage/BMQDH3XK/Decker et al_2017_Rapidly Adjustable Non-intrusive Online Monitoring for Multi-core Systems.pdf}
}

@inproceedings{dengFlexibleEfficientInstructionGrained2010,
  title = {Flexible and {{Efficient Instruction-Grained Run-Time Monitoring Using On-Chip Reconfigurable Fabric}}},
  booktitle = {2010 43rd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}},
  author = {Deng, D. Y. and Lo, D. and Malysa, G. and Schneider, S. and Suh, G. E.},
  year = {2010},
  month = dec,
  pages = {137--148},
  issn = {2379-3155},
  doi = {10.1109/MICRO.2010.17},
  abstract = {This paper proposes Flex Core, a hybrid processor architecture where an on-chip reconfigurable fabric (FPGA) is tightly coupled with the main processing core. Flex Core provides an efficient platform that can support a broad range of run-time monitoring and bookkeeping techniques. Unlike using custom hardware, which is more efficient but often extremely difficult and expensive to incorporate into a modern microprocessor, the Flex Core architecture allows parallel monitoring and bookkeeping functions to be dynamically added to the processing core and adapt to application needs even after the chip has been fabricated. At the same time, Flex Core is far more efficient than software implementations because its fine-grained reconfigurable architecture closely matches bit level operations of typical monitoring schemes and allows monitoring schemes to operate in parallel to the monitored core. In fact, our experimental results show that monitoring on Flex Core can almost match the performance of full ASIC implementations. To evaluate the Flex Core architecture, we implemented an RTL prototype along with several extensions including uninitialized memory read checking, dynamic information flow tracking, array bound checking, and soft error checking. The prototypes demonstrate that the architecture can support a range of monitoring extensions with different characteristics in an efficient manner. Flex Core takes moderate silicon area and results in far better performance and energy efficiency than software.},
  keywords = {application specific integrated circuits,ASIC,bit- level operations,bookkeeping techniques,chip fabrication,Computer architecture,Coprocessing Architecture,custom hardware,energy efficiency,Fabrics,field programmable gate arrays,fine-grained reconfigurable architecture,FlexCore architecture,FPGA,Hardware,hybrid processor architecture,instruction-grained run-time monitoring,main processing core,microprocessor chips,Monitoring,multiprocessing systems,on-chip reconfigurable fabric,Prototypes,Reconfigurability,reconfigurable architectures,Registers,Reliability,RTL prototype,Security,silicon area,Software,software implementations},
  file = {/home/fabian/Zotero/storage/Z2MD4CIU/Deng et al_2010_Flexible and Efficient Instruction-Grained Run-Time Monitoring Using On-Chip.pdf;/home/fabian/Zotero/storage/PR87XFCV/5695532.html}
}

@inproceedings{deoliveiracastroFinegrainedBenchmarkSubsetting2014,
  title = {Fine-Grained {{Benchmark Subsetting}} for {{System Selection}}},
  booktitle = {Proceedings of {{Annual IEEE}}/{{ACM International Symposium}} on {{Code Generation}} and {{Optimization}}},
  author = {{de Oliveira Castro}, Pablo and Kashnikov, Yuriy and Akel, Chadi and Popov, Mihail and Jalby, William},
  year = {2014},
  month = feb,
  series = {{{CGO}} '14},
  pages = {132--142},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2581122.2544144},
  abstract = {System selection aims at finding the best architecture for a set of programs and workloads. It traditionally requires long running benchmarks. We propose a method to reduce the cost of system selection. We break down benchmarks into elementary fragments of source code, called codelets. Then, we identify two causes of redundancy: first, similar codelets; second, codelets called repeatedly. The key idea is to minimize redundancy inside the benchmark suite to speed it up. For each group of similar codelets, only one representative is kept. For codelets called repeatedly and for which the performance does not vary across calls, the number of invocations is reduced. Given an initial benchmark suite, our method produces a set of reduced benchmarks that can be used in place of the original one for system selection. We evaluate our method on the NAS SER benchmarks, producing a reduced benchmark suite 30 times faster in average than the original suite, with a maximum of 44 times. The reduced suite predicts the execution time on three target architectures with a median error between 3.9\% and 8\%.},
  isbn = {978-1-4503-2670-4},
  file = {/home/fabian/Zotero/storage/XZ9Z3EME/de Oliveira Castro et al_2014_Fine-grained Benchmark Subsetting for System Selection.pdf}
}

@article{dixitSPECBenchmarks1991,
  title = {The {{SPEC}} Benchmarks},
  author = {Dixit, Kaivalya M.},
  year = {1991},
  month = dec,
  journal = {Parallel Computing},
  series = {Benchmarking of High Performance Supercomputers},
  volume = {17},
  number = {10},
  pages = {1195--1209},
  issn = {0167-8191},
  doi = {10.1016/S0167-8191(05)80033-X},
  abstract = {This paper characterizes problems with popular benchmarks used to measure system performance and describes the history, structure, mission, future, and workings of an evolving standard in characterizing systems performance. Systems Performance Evaluation Cooperative (SPEC) represents an effort of major computer companies to create a yardstick to measure the performance of computer systems. This paper compares results of three architectures with traditional performance metrics and metrics developed by SPEC. It discusses strengths and weaknesses of SPEC and warns users about the dangers of single number performance characterization.},
  langid = {english},
  keywords = {Benchmarks,performance metric,performance results,SPEC,system performance},
  file = {/home/fabian/Zotero/storage/FDPWENMY/Dixit_1991_The SPEC benchmarks.pdf;/home/fabian/Zotero/storage/RBHFND8M/S016781910580033X.html}
}

@article{dujmovicEvolutionEvaluationSPEC1998,
  title = {Evolution and Evaluation of {{SPEC}} Benchmarks},
  author = {Dujmovic, Jozo J. and Dujmovic, Ivo},
  year = {1998},
  month = dec,
  journal = {ACM SIGMETRICS Performance Evaluation Review},
  volume = {26},
  number = {3},
  pages = {2--9},
  issn = {0163-5999},
  doi = {10.1145/306225.306228},
  abstract = {We present a method for quantitative evaluation of SPEC benchmarks. The method is used for the analysis of three generations of SPEC component-level benchmarks: SPEC89, SPEC92, and SPEC95. Our approach is suitable for studying (1) the redundancy between individual benchmark programs, (2) the size, completeness, density and granularity of benchmark suites, (3) the distribution of benchmark programs in a program space, and (4) benchmark suite design and evolution strategies. The presented method can be used for designing a universal benchmark suite as the next generation of SPEC benchmarks.},
  keywords = {hardware approach},
  file = {/home/fabian/Zotero/storage/7Y3NICZX/Dujmovic_Dujmovic_1998_Evolution and evaluation of SPEC benchmarks.pdf}
}

@inproceedings{dujmovicQuantitativeMethodsDesign1996,
  title = {Quantitative Methods for Design of Benchmark Suites},
  booktitle = {Proceedings of {{MASCOTS}} '96 - 4th {{International Workshop}} on {{Modeling}}, {{Analysis}} and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}}},
  author = {Dujmovic, J.J.},
  year = {1996},
  month = feb,
  pages = {162--166},
  doi = {10.1109/MASCOT.1996.501011},
  abstract = {We present black-box models of workload difference and we then to develop quantitative methods for the design of benchmark suites. The proposed design methods (the method of optimum subsets, and the method of optimum weights) combine a new concept of program space coverage functions, and the criteria of completeness, redundancy, density, and granularity of benchmark suites.},
  keywords = {Aerospace industry,Computer industry,Computer network reliability,Computer networks,Computer science,Costs,Design methodology,Load modeling,Redundancy,Throughput},
  file = {/home/fabian/Zotero/storage/28HB6NUN/Dujmovic_1996_Quantitative methods for design of benchmark suites.pdf;/home/fabian/Zotero/storage/5NIZM9CG/501011.html}
}

@article{eeckhoutDesigningComputerArchitecture2003,
  title = {Designing Computer Architecture Research Workloads},
  author = {Eeckhout, L. and Vandierendonck, H. and Bosschere, K. De},
  year = {2003},
  month = feb,
  journal = {Computer},
  volume = {36},
  number = {2},
  pages = {65--71},
  issn = {1558-0814},
  doi = {10.1109/MC.2003.1178050},
  abstract = {Although architectural simulators model microarchitectures at a high abstraction level, the increasing complexity of both the microarchitectures themselves and the applications that run on them make simulator use extremely time-consuming. Simulators must execute huge numbers of instructions to create a workload representative of real applications, creating an unreasonably long simulation time and stretching the time to market. Using reduced input sets instead of reference input sets helps to solve this problem. The authors have developed a methodology that reliably quantifies program behavior similarity to verify if reduced input sets result in program behavior similar to the reference inputs.},
  keywords = {architectural simulators,Computational modeling,computer architecture,Computer architecture,computer architecture research workloads,Costs,Data analysis,Hardware,Microarchitecture,microarchitectures,microprocessor chips,microprocessor design,Microprocessors,MinneSPEC,Performance analysis,performance evaluation,principal component analysis,program behavior,reduced input sets,reference input sets,software approach,Thumb,time to market,Time to market,virtual machines},
  file = {/home/fabian/Zotero/storage/KHFHDEFJ/Eeckhout et al_2003_Designing computer architecture research workloads.pdf;/home/fabian/Zotero/storage/RDASW2LC/1178050.html}
}

@article{eeckhoutQuantifyingImpactInput,
  title = {Quantifying the {{Impact}} of {{Input Data Sets}} on {{Program Behavior}} and Its {{Applications}}},
  author = {Eeckhout, Lieven and Vandierendonck, Hans},
  pages = {33},
  abstract = {Having a representative workload of the target domain of a microprocessor is extremely important throughout its design. The composition of a workload involves two issues: (i) which benchmarks to select and (ii) which input data sets to select per benchmark. Unfortunately, it is impossible to select a huge number of benchmarks and respective input sets due to the large instruction counts per benchmark and due to limitations on the available simulation time. In this paper, we use statistical data analysis techniques such as principal components analysis (PCA) and cluster analysis to efficiently explore the workload space. Within this workload space, different input data sets for a given benchmark can be displayed, a distance can be measured between program-input pairs that gives us an idea about their mutual behavioral differences and representative input data sets can be selected for the given benchmark. This methodology is validated by showing that program-input pairs that are close to each other in this workload space indeed exhibit similar behavior. The final goal is to select a limited set of representative benchmark-input pairs that span the complete workload space. Next to workload composition, we discuss two other possible applications, namely getting insight in the impact of input data sets on program behavior and evaluating the representativeness of sampled traces.},
  langid = {english},
  keywords = {hybrid approach},
  file = {/home/fabian/Zotero/storage/GUPMWTNT/Eeckhout and Vandierendonck - Quantifying the Impact of Input Data Sets on Progr.pdf}
}

@article{fogartyOnchipSupportSoftware2013,
  title = {On-Chip Support for Software Verification and Debug in Multi-Core Embedded Systems},
  author = {Fogarty, Padraig and MacNamee, Ciaran and Heffernan, Donal},
  year = {2013},
  journal = {IET Software},
  volume = {7},
  number = {1},
  pages = {56--64},
  issn = {1751-8814},
  doi = {10.1049/iet-sen.2011.0212},
  abstract = {The challenges in silicon testing and debug of complex integrated circuits are well understood. Where these circuits include multiple processor cores there is also a dramatic increase in the complexity of verifying and debugging the associated software; with much of this complexity being because of the inherent lack of visibility over internal signals which integration brings. The trend to-date has been to rely upon silicon test interfaces to provide access to internal signals required for software verification and debug. However, it is questionable whether this is sufficient for real-time systems or future designs with increasing processor cores. This study examines the on-chip technology supporting software verification and debug in current designs and proposes enhancements in this area. As much of this technology is primarily intended for silicon test it is lacking in terms of I/O bandwidth, which is a significant limitation for software verification and debug. The authors propose their alternative approach of using an on-chip coprocessor and debug circuitry to address this principal limitation; and describe an embedded application where this approach was successfully applied to monitor timing requirements and detect failures. The authors also outline how this approach could be applied as an architectural solution for formal runtime verification.},
  copyright = {\textcopyright{} 2013 The Institution of Engineering and Technology},
  langid = {english},
  keywords = {complex integrated circuit debugging,coprocessors,debug circuitry,embedded systems,failure detection,fault tolerant computing,formal runtime verification,integrated circuit testing,multicore embedded systems,multiple processor cores,multiprocessing systems,on-chip coprocessor,program debugging,program verification,real-time systems,silicon test interfaces,software debugging complexity,software verification,system-on-chip,timing,timing requirement monitoring},
  annotation = {\_eprint: https://ietresearch.onlinelibrary.wiley.com/doi/pdf/10.1049/iet-sen.2011.0212},
  file = {/home/fabian/Zotero/storage/UCPWUXUU/Fogarty et al_2013_On-chip support for software verification and debug in multi-core embedded.pdf;/home/fabian/Zotero/storage/CCGPNG4H/iet-sen.2011.html}
}

@inproceedings{gajFairComprehensiveMethodology2010,
  title = {Fair and {{Comprehensive Methodology}} for {{Comparing Hardware Performance}} of {{Fourteen Round Two SHA-3 Candidates Using FPGAs}}},
  booktitle = {Cryptographic {{Hardware}} and {{Embedded Systems}}, {{CHES}} 2010},
  author = {Gaj, Kris and Homsirikamol, Ekawat and Rogawski, Marcin},
  editor = {Mangard, Stefan and Standaert, Fran{\c c}ois-Xavier},
  year = {2010},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {264--278},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15031-9_18},
  abstract = {Performance in hardware has been demonstrated to be an important factor in the evaluation of candidates for cryptographic standards. Up to now, no consensus exists on how such an evaluation should be performed in order to make it fair, transparent, practical, and acceptable for the majority of the cryptographic community. In this paper, we formulate a proposal for a fair and comprehensive evaluation methodology, and apply it to the comparison of hardware performance of 14 Round 2 SHA-3 candidates. The most important aspects of our methodology include the definition of clear performance metrics, the development of a uniform and practical interface, generation of multiple sets of results for several representative FPGA families from two major vendors, and the application of a simple procedure to convert multiple sets of results into a single ranking.},
  isbn = {978-3-642-15031-9},
  langid = {english},
  keywords = {benchmarking,FPGA,hash functions,SHA-3},
  file = {/home/fabian/Zotero/storage/N5LD6AWX/Gaj et al_2010_Fair and Comprehensive Methodology for Comparing Hardware Performance of.pdf}
}

@article{gal-onExploringCoremarkBenchmark2012,
  title = {Exploring Coremark a Benchmark Maximizing Simplicity and Efficacy},
  author = {{Gal-On}, Shay and Levy, Markus},
  year = {2012},
  journal = {The Embedded Microprocessor Benchmark Consortium},
  file = {/home/fabian/Zotero/storage/NCHWB2IZ/Gal-On and Levy - 2012 - Exploring coremark a benchmark maximizing simplici.pdf}
}

@article{gautschiNearThresholdRISCVCore2017,
  title = {Near-{{Threshold RISC-V Core With DSP Extensions}} for {{Scalable IoT Endpoint Devices}}},
  author = {Gautschi, Michael and Schiavone, Pasquale Davide and Traber, Andreas and Loi, Igor and Pullini, Antonio and Rossi, Davide and Flamand, Eric and G{\"u}rkaynak, Frank K. and Benini, Luca},
  year = {2017},
  month = oct,
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {25},
  number = {10},
  pages = {2700--2713},
  issn = {1557-9999},
  doi = {10.1109/TVLSI.2017.2654506},
  abstract = {Endpoint devices for Internet-of-Things not only need to work under extremely tight power envelope of a few milliwatts, but also need to be flexible in their computing capabilities, from a few kOPS to GOPS. Near-threshold (NT) operation can achieve higher energy efficiency, and the performance scalability can be gained through parallelism. In this paper, we describe the design of an open-source RISC-V processor core specifically designed for NT operation in tightly coupled multicore clusters. We introduce instruction extensions and microarchitectural optimizations to increase the computational density and to minimize the pressure toward the shared-memory hierarchy. For typical data-intensive sensor processing workloads, the proposed core is, on average, 3.5\texttimes{} faster and 3.2\texttimes{} more energy efficient, thanks to a smart L0 buffer to reduce cache access contentions and support for compressed instructions. Single Instruction Multiple Data extensions, such as dot products, and a built-in L0 storage further reduce the shared-memory accesses by 8\texttimes{} reducing contentions by 3.2\texttimes. With four NT-optimized cores, the cluster is operational from 0.6 to 1.2 V, achieving a peak efficiency of 67 MOPS/mW in a low-cost 65-nm bulk CMOS technology. In a low-power 28-nm FD-SOI process, a peak efficiency of 193 MOPS/mW (40 MHz and 1 mW) can be achieved.},
  keywords = {Feature extraction,Hardware,Instruction set architecture (ISA) extensions,Internet-of-Things,multicore,Multicore processing,Performance evaluation,Process control,RISC-V,Standards,ultralow power (ULP)},
  file = {/home/fabian/Zotero/storage/JG832Y28/Gautschi et al_2017_Near-Threshold RISC-V Core With DSP Extensions for Scalable IoT Endpoint Devices.pdf;/home/fabian/Zotero/storage/YGGB78H7/7864441.html}
}

@article{gouldNotePerformanceProfiles2016,
  title = {A {{Note}} on {{Performance Profiles}} for {{Benchmarking Software}}},
  author = {Gould, Nicholas and Scott, Jennifer},
  year = {2016},
  month = aug,
  journal = {ACM Transactions on Mathematical Software},
  volume = {43},
  number = {2},
  pages = {15:1--15:5},
  issn = {0098-3500},
  doi = {10.1145/2950048},
  abstract = {In recent years, performance profiles have become a popular and widely used tool for benchmarking and evaluating the performance of several solvers when run on a large test set. Here we use data from a real application as well as a simple artificial example to illustrate that caution should be exercised when trying to interpret performance profiles to assess the relative performance of the solvers.},
  keywords = {benchmarking,Performance profiles,testing},
  file = {/home/fabian/Zotero/storage/TVEV8LFM/Gould_Scott_2016_A Note on Performance Profiles for Benchmarking Software.pdf}
}

@inproceedings{guthausMiBenchFreeCommercially2001,
  title = {{{MiBench}}: {{A}} Free, Commercially Representative Embedded Benchmark Suite},
  shorttitle = {{{MiBench}}},
  booktitle = {Proceedings of the {{Fourth Annual IEEE International Workshop}} on {{Workload Characterization}}. {{WWC-4}} ({{Cat}}. {{No}}.{{01EX538}})},
  author = {Guthaus, M.R. and Ringenberg, J.S. and Ernst, D. and Austin, T.M. and Mudge, T. and Brown, R.B.},
  year = {2001},
  month = dec,
  pages = {3--14},
  doi = {10.1109/WWC.2001.990739},
  abstract = {This paper examines a set of commercially representative embedded programs and compares them to an existing benchmark suite, SPEC2000. A new version of SimpleScalar that has been adapted to the ARM instruction set is used to characterize the performance of the benchmarks using configurations similar to current and next generation embedded processors. Several characteristics distinguish the representative embedded programs from the existing SPEC benchmarks including instruction distribution, memory behavior, and available parallelism. The embedded benchmarks, called MiBench, are freely available to all researchers.},
  keywords = {Application software,Code standards,Computer science,Digital audio players,Instruction sets,Microcontrollers,Microprocessors,Multimedia systems,Parallel processing,Process design},
  file = {/home/fabian/Zotero/storage/7W9THIBY/Guthaus et al_2001_MiBench.pdf;/home/fabian/Zotero/storage/9NIW8RYI/990739.html}
}

@inproceedings{hauserGarpMIPSProcessor1997,
  title = {Garp: A {{MIPS}} Processor with a Reconfigurable Coprocessor},
  shorttitle = {Garp},
  booktitle = {Proceedings. {{The}} 5th {{Annual IEEE Symposium}} on {{Field-Programmable Custom Computing Machines Cat}}. {{No}}.{{97TB100186}})},
  author = {Hauser, J. R. and Wawrzynek, J.},
  year = {1997},
  month = apr,
  pages = {12--21},
  doi = {10.1109/FPGA.1997.624600},
  abstract = {Typical reconfigurable machines exhibit shortcomings that make them less than ideal for general-purpose computing. The Garp Architecture combines reconfigurable hardware with a standard MIPS processor on the same die to retain the better features of both. Novel aspects of the architecture are presented, as well as a prototype software environment and preliminary performance results. Compared to an UltraSPARC, a Garp of similar technology could achieve speedups ranging from a factor of 2 to as high as a factor of 24 for some useful applications.},
  keywords = {Application software,Circuits,Computer architecture,coprocessors,Coprocessors,field programmable gate arrays,Field programmable gate arrays,FPGA,Garp Architecture,general purpose computers,general-purpose computing,Hardware,instruction sets,microprocessor chips,MIPS processor,performance,performance evaluation,prototype software environment,reconfigurable architectures,reconfigurable coprocessor,Reconfigurable logic,reconfigurable machines,Software performance,Software prototyping,speedups,Switches,UltraSPARC},
  file = {/home/fabian/Zotero/storage/YZCD4763/Hauser_Wawrzynek_1997_Garp.pdf;/home/fabian/Zotero/storage/YHKY5ERM/624600.html}
}

@article{henningSPECCPU2000Measuring2000,
  title = {{{SPEC CPU2000}}: Measuring {{CPU}} Performance in the {{New Millennium}}},
  shorttitle = {{{SPEC CPU2000}}},
  author = {Henning, J. L.},
  year = {2000},
  month = jul,
  journal = {Computer},
  volume = {33},
  number = {7},
  pages = {28--35},
  issn = {1558-0814},
  doi = {10.1109/2.869367},
  abstract = {As computers and software have become more powerful, it seems almost human nature to want the biggest and fastest toy you can afford. But how do you know if your toy is tops? Even if your application never does any I/O, it's not just the speed of the CPU that dictates performance. Cache, main memory, and compilers also play a role. Software applications also have differing performance requirements. So whom do you trust to provide this information? The Standard Performance Evaluation Corporation (SPEC) is a nonprofit consortium whose members include hardware vendors, software vendors, universities, customers, and consultants. SPEC's mission is to develop technically credible and objective component- and system-level benchmarks for multiple operating systems and environments, including high-performance numeric computing, Web servers, and graphical subsystems. On 30 June 2000, SPEC retired the CPU95 benchmark suite. Its replacement is CPU2000, a new CPU benchmark suite with 19 applications that have never before been in a SPEC CPU suite. The article discusses how SPEC developed this benchmark suite and what the benchmarks do.},
  keywords = {Application software,Benchmark testing,computer evaluation,CPU benchmark suite,CPU performance measurement,CPU95 benchmark suite,DP industry,Educational institutions,graphical subsystems,Hardware,high-performance numeric computing,Humans,multiple operating systems,nonprofit consortium,Operating systems,performance evaluation,performance requirements,program testing,Software performance,Software standards,SPEC CPU2000,Standard Performance Evaluation Corporation,system-level benchmarks,Web server,Web servers,Workstations},
  file = {/home/fabian/Zotero/storage/CNA9DDV7/Henning_2000_SPEC CPU2000.pdf;/home/fabian/Zotero/storage/U6LSZDYE/869367.html}
}

@article{HiBenchBenchmarkSuite,
  title = {The {{HiBench Benchmark Suite}}: {{Characterization}} of the {{MapReduce-Based Data Analysis}}},
  pages = {2},
  langid = {english},
  file = {/home/fabian/Zotero/storage/TE452N4F/The HiBench Benchmark Suite Characterization of t.pdf}
}

@inproceedings{hostePerformancePredictionBased2006,
  title = {Performance Prediction Based on Inherent Program Similarity},
  booktitle = {2006 {{International Conference}} on {{Parallel Architectures}} and {{Compilation Techniques}} ({{PACT}})},
  author = {Hoste, Kenneth and Phansalkar, Aashish and Eeckhout, Lieven and Georges, Andy and John, Lizy K. and De Bosschere, Koen},
  year = {2006},
  month = sep,
  pages = {114--122},
  abstract = {A key challenge in benchmarking is to predict the performance of an application of interest on a number of platforms in order to determine which platform yields the best performance. This paper proposes an approach for doing this. We measure a number of microarchitecture-independent characteristics from the application of interest, and relate these characteristics to the characteristics of the programs from a previously profiled benchmark suite. Based on the similarity of the application of interest with programs in the benchmark suite, we make a performance prediction of the application of interest. We propose and evaluate three approaches (normalization, principal components analysis and genetic algorithm) to transform the raw data set of microarchitecture-independent characteristics into a benchmark space in which the relative distance is a measure for the relative performance differences. We evaluate our approach using all of the SPEC CPU2000 benchmarks and real hardware performance numbers from the SPEC website. Our framework estimates per-benchmark machine ranks with a 0.89 average and a 0.80 worst case rank correlation coefficient.},
  keywords = {Benchmark testing,History,Inherent Program Behavior,Markov processes,Microarchitecture,Performance Modeling,Program processors,Registers,Transforms,Workload Characterization},
  file = {/home/fabian/Zotero/storage/IBH6WB43/Hoste et al_2006_Performance prediction based on inherent program similarity.pdf;/home/fabian/Zotero/storage/Q4C23WA6/7847586.html}
}

@article{jararwehHardwarePerformanceEvaluation2012,
  title = {Hardware {{Performance Evaluation}} of {{SHA-3 Candidate Algorithms}}},
  author = {Jararweh, Yaser and Tawalbeh, Lo'ai and Tawalbeh, Hala and Moh'd, Abidalrahman},
  year = {2012},
  month = apr,
  volume = {2012},
  publisher = {{Scientific Research Publishing}},
  doi = {10.4236/jis.2012.32008},
  abstract = {Secure Hashing Algorithms (SHA) showed a significant importance in today's information security applications. The National Institute of Standards and Technology (NIST), held a competition of three rounds to replace SHA1 and SHA2 with the new SHA-3, to ensure long term robustness of hash functions. In this paper, we present a comprehensive hardware evaluation for the final round SHA-3 candidates. The main goal of providing the hardware evaluation is to: find the best algorithm among them that will satisfy the new hashing algorithm standards defined by the NIST. This is based on a comparison made between each of the finalists in terms of security level, throughput, clock frequancey, area, power consumption, and the cost. We expect that the achived results of the comparisons will contribute in choosing the next hashing algorithm (SHA-3) that will support the security requirements of applications in todays ubiquitous and pervasive information infrastructure.},
  langid = {english},
  file = {/home/fabian/Zotero/storage/PUUJ89CY/Jararweh et al_2012_Hardware Performance Evaluation of SHA-3 Candidate Algorithms.pdf;/home/fabian/Zotero/storage/ZYIFGEVQ/1-7800059_18767.html}
}

@incollection{jerrayaPerformanceEvaluationMethods2006,
  title = {Performance {{Evaluation Methods}} for {{Multiprocessor System-on-Chip Design}}},
  booktitle = {{{EDA}} for {{IC System Design}}, {{Verification}}, and {{Testing}}},
  editor = {Jerraya, Ahmed and Bacivarov, Iuliana},
  year = {2006},
  publisher = {{CRC Press}},
  abstract = {Multi-processor systems-on-chip (MPSoCs) require the integration of heterogeneous components on a single chip. This chapter provides an overview of the performance evaluation methods developed for specific subsystems. It proposes to combine subsystem performance evaluation methods to deal with MPSoC. MPSoC is an emerging community trying to integrate multiple subsystems on a single chip and consequently requiring new methods for performance evaluation. The chapter explains the key characteristics of performance evaluation environments. It discusses the analyzed subsystems, their description models and environments, and the associated performance evaluation tools and methods. The chapter proposes several trends that could guide future research toward building efficient MPSoC performance evaluation environments. Performance evaluation is the process that analyzes the capabilities of a system in a particular context, i.e., a given behavior, a specific load, or a specific set of inputs. Three major axes define existing performance evaluation tools: the subsystem under analysis, the performance model, and the performance evaluation methodology.},
  isbn = {978-1-315-22170-0}
}

@inproceedings{jindalDHOOMReusingDesignforDebug2019,
  title = {{{DHOOM}}: {{Reusing Design-for-Debug Hardware}} for {{Online Monitoring}}},
  shorttitle = {{{DHOOM}}},
  booktitle = {2019 56th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Jindal, N. and Chandran, S. and Panda, P. R. and Prasad, S. and Mitra, A. and Singhal, K. and Gupta, S. and Tuli, S.},
  year = {2019},
  month = jun,
  pages = {1--6},
  issn = {0738-100X},
  abstract = {Runtime verification employs dedicated hardware or software monitors to check whether program properties hold at runtime. However, these monitors often incur high area and performance overheads depending on whether they are implemented in hardware or software. In this work, we propose DHOOM, an architectural framework for runtime monitoring of program assertions, which exploits the combination of a reconfigurable fabric present alongside a processor core with the vestigial on-chip Design-for-Debug hardware. This combination of hardware features allows DHOOM to minimize the overall performance overhead of runtime verification, even when subject to a given area constraint. We present an algorithm for dynamically selecting an effective subset of assertion monitors that can be accommodated in the available programmable fabric, while instrumenting the remaining assertions in software. We show that our proposed strategy, while respecting area constraints, reduces the performance overhead of runtime verification by up to 32\% when compared with a baseline of software-only monitors.},
  keywords = {Design-for-Debug Hardware,DH-HEMTs,DHOOM,Fabrics,Hardware,Monitoring,online monitoring,program debugging,program properties,program verification,reconfigurable architectures,reconfigurable fabric,Registers,Runtime,runtime monitoring,Runtime Monitoring,runtime verification,Software,vestigial on-chip design-for-debug hardware},
  file = {/home/fabian/Zotero/storage/PR3NUXL9/Jindal et al. - 2019 - DHOOM Reusing Design-for-Debug Hardware for Onlin.pdf;/home/fabian/Zotero/storage/EP3Z3VUH/8806785.html}
}

@book{johnsonSuperscalarMicroprocessorDesign1991,
  title = {Superscalar {{Microprocessor Design}}},
  author = {Johnson, Mike},
  year = {1991},
  file = {/home/fabian/Zotero/storage/I4JZUGEH/10006487821.html}
}

@article{joshiDistillingEssenceProprietary2008,
  title = {Distilling the Essence of Proprietary Workloads into Miniature Benchmarks},
  author = {Joshi, Ajay and Eeckhout, Lieven and Bell, Robert H. and John, Lizy K.},
  year = {2008},
  month = sep,
  journal = {ACM Transactions on Architecture and Code Optimization},
  volume = {5},
  number = {2},
  pages = {10:1--10:33},
  issn = {1544-3566},
  doi = {10.1145/1400112.1400115},
  abstract = {Benchmarks set standards for innovation in computer architecture research and industry product development. Consequently, it is of paramount importance that these workloads are representative of real-world applications. However, composing such representative workloads poses practical challenges to application analysis teams and benchmark developers (1) real-world workloads are intellectual property and vendors hesitate to share these proprietary applications; and (2) porting and reducing these applications to benchmarks that can be simulated in a tractable amount of time is a nontrivial task. In this paper, we address this problem by proposing a technique that automatically distills key inherent behavioral attributes of a proprietary workload and captures them into a miniature synthetic benchmark clone. The advantage of the benchmark clone is that it hides the functional meaning of the code but exhibits similar performance characteristics as the target application. Moreover, the dynamic instruction count of the synthetic benchmark clone is substantially shorter than the proprietary application, greatly reducing overall simulation time for SPEC CPU, the simulation time reduction is over five orders of magnitude compared to entire benchmark execution. Using a set of benchmarks representative of general-purpose, scientific, and embedded applications, we demonstrate that the power and performance characteristics of the synthetic benchmark clone correlate well with those of the original application across a wide range of microarchitecture configurations.},
  keywords = {benchmark cloning,Benchmarks,workload characterization},
  file = {/home/fabian/Zotero/storage/PJX9TL6G/Joshi et al_2008_Distilling the essence of proprietary workloads into miniature benchmarks.pdf}
}

@article{joshiMeasuringBenchmarkSimilarity2006,
  title = {Measuring Benchmark Similarity Using Inherent Program Characteristics},
  author = {Joshi, Ajay and Phansalkar, Aashish and Eeckhout, L. and John, L.K.},
  year = {2006},
  month = jun,
  journal = {IEEE Transactions on Computers},
  volume = {55},
  number = {6},
  pages = {769--782},
  issn = {1557-9956},
  doi = {10.1109/TC.2006.85},
  abstract = {This paper proposes a methodology for measuring the similarity between programs based on their inherent microarchitecture-independent characteristics, and demonstrates two applications for it: 1) finding a representative subset of programs from benchmark suites and 2) studying the evolution of four generations of SPEC CPU benchmark suites. Using the proposed methodology, we find a representative subset of programs from three popular benchmark suites - SPEC CPU2000, MediaBench, and MiBench. We show that this subset of representative programs can be effectively used to estimate the average benchmark suite IPC, L1 data cache miss-rates, and speedup on 11 machines with different ISAs and microarchitectures - this enables one to save simulation time with little loss in accuracy. From our study of the similarity between the four generations of SPEC CPU benchmark suites, we find that, other than a dramatic increase in the dynamic instruction count and increasingly poor temporal data locality, the inherent program characteristics have more or less remained unchanged.},
  keywords = {Measurement techniques,modeling techniques,performance attributes.,performance of systems,Software performance},
  file = {/home/fabian/Zotero/storage/8GRGLMXN/Joshi et al_2006_Measuring benchmark similarity using inherent program characteristics.pdf;/home/fabian/Zotero/storage/X8EBEGM6/1628963.html}
}

@article{kaoHardwareApproachRealTime2007,
  title = {A {{Hardware Approach}} to {{Real-Time Program Trace Compression}} for {{Embedded Processors}}},
  author = {Kao, C. and Huang, S. and Huang, I.},
  year = {2007},
  month = mar,
  journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {54},
  number = {3},
  pages = {530--543},
  issn = {1558-0806},
  doi = {10.1109/TCSI.2006.887613},
  abstract = {Collecting the program execution traces at full speed is essential to the analysis and debugging of real-time software behavior of a complex system. However, the generation rate and the size of real-time program traces are so huge such that real-time program tracing is often infeasible without proper hardware support. This paper presents a hardware approach to compress program execution traces in real time in order to reduce the trace size. The approach consists of three modularized phases: 1) branch/target filtering; 2) branch/target address encoding; 3) Lempel-Ziv (LZ)-based data compression. A synthesizable RTL code for the proposed hardware is constructed to analyze the hardware cost and speed and typical multimedia benchmarks are used to measure the compression results. The results show that our hardware is capable of real-time compression and achieving compression ratio of 454:1, far better than 5:1 achieved by typical existing hardware approaches. Furthermore, our modularized approach makes it possible to trade off between the hardware cost (typically from 1 to 50K gates) and the achievable compression ratio (typically from 5:1 to 454:1)},
  keywords = {Address trace,branch/target address encoding,branch/target filtering,compressor,Computer science,Costs,Data acquisition,data compression,Data compression,debugging,done,embedded processors,embedded systems,Encoding,Filtering,Hardware,hardware-software codesign,Lempel-Ziv-based data compression,microprocessor,microprocessor chips,Microprocessors,program execution traces,real time,Real time systems,real-time program trace compression,RTL code,Software debugging},
  file = {/home/fabian/Zotero/storage/42S7DT2X/kao2007.pdf;/home/fabian/Zotero/storage/QR4M24R8/4126796.html}
}

@inproceedings{kimMeasuringSimilarityWindows2013,
  title = {Measuring Similarity of Windows Applications Using Static and Dynamic Birthmarks},
  booktitle = {Proceedings of the 28th {{Annual ACM Symposium}} on {{Applied Computing}}},
  author = {Kim, Dongjin and Han, Yongman and Cho, Seong-je and Yoo, Haeyoung and Woo, Jinwoon and Nah, Yunmook and Park, Minkyu and Chung, Lawrence},
  year = {2013},
  month = mar,
  series = {{{SAC}} '13},
  pages = {1628--1633},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2480362.2480666},
  abstract = {A software birthmark is unique, as certain native characteristics of a program, hence can be used to measure the similarity between programs. In general, a static software birthmark does not need program execution, but is more vulnerable to attacks by semantic-preserving transformations. A dynamic software birthmark is applicable to packed executables, but cannot cover all the possible program paths. In this paper, we propose a novel effective technique to measure the similarity of Microsoft Windows applications using both static and dynamic birthmarks, which are based on the list of system APIs as well as the frequency of system API calls. Because system APIs are located in Windows system directories and act as a bridge between applications and the operating system, our birthmarks are resilient to obfuscations and compiler optimizations. A static birthmark consists of the system API call frequency of a target program, which can be extracted by scanning the executable file. A dynamic birthmark is the frequency of system API function calls, which can be extracted by a binary instrumentation tool during the execution of the program. To evaluate the effectiveness of the proposed technique, we compare various types of Windows applications using both the static and dynamic birthmarks. To demonstrate the robustness, we compare packed executables that were compressed by a binary packing tool. We carry out additional experiments for measuring the similarity between target Windows applications at the source code level and verify the evaluation results. The experimental results show that our birthmarks can effectively measure the similarity between Windows applications, as intended.},
  isbn = {978-1-4503-1656-9},
  keywords = {dynamic birthmark,software similarity,static birthmark,system API,windows application},
  file = {/home/fabian/Zotero/storage/8G4BPZ2T/Kim et al_2013_Measuring similarity of windows applications using static and dynamic birthmarks.pdf}
}

@inproceedings{liTracebasedMicroarchitecturelevelDiagnosis2008,
  title = {Trace-Based Microarchitecture-Level Diagnosis of Permanent Hardware Faults},
  booktitle = {2008 {{IEEE International Conference}} on {{Dependable Systems}} and {{Networks With FTCS}} and {{DCC}} ({{DSN}})},
  author = {Li, M. and Ramachandran, P. and Sahoo, S. K. and Adve, S. V. and Adve, V. S. and {Yuanyuan Zhou}},
  year = {2008},
  month = jun,
  pages = {22--31},
  issn = {2158-3927},
  doi = {10.1109/DSN.2008.4630067},
  abstract = {As devices continue to scale, future shipped hardware will likely fail due to in-the-field hardware faults. As traditional redundancy-based hardware reliability solutions that tackle these faults will be too expensive to be broadly deployable, recent research has focused on low-overhead reliability solutions. One approach is to employ low-overhead (ldquoalways-onrdquo) detection techniques that catch high-level symptoms and pay a higher overhead for (rarely invoked) diagnosis. This paper presents trace-based fault diagnosis, a diagnosis strategy that identifies permanent faults in microarchitectural units by analyzing the faulty corepsilas instruction trace. Once a fault is detected, the faulty core is rolled back and re-executes from a previous checkpoint, generating a faulty instruction trace and recording the microarchitecture-level resource usage. A diagnosis process on another fault-free core then generates a fault-free trace which it compares with the faulty trace to identify the faulty unit. Our result shows that this approach successfully diagnoses 98\% of the faults studied and is a highly robust and flexible way for diagnosing permanent faults.},
  keywords = {checkpointing,Circuit faults,computer architecture,fault diagnosis,Fault diagnosis,fault tolerance,Hardware,instruction sets,instruction trace-based microarchitecture-level fault diagnosis,logic design,logic testing,Microarchitecture,microarchitecture-level resource usage,microprocessor chips,Microprogramming,permanent hardware fault,processor-level redundancy-based hardware reliability solution,Radiation detectors,Registers},
  file = {/home/fabian/Zotero/storage/MBIFEG86/Li et al. - 2008 - Trace-based microarchitecture-level diagnosis of p.pdf;/home/fabian/Zotero/storage/87EIQKQJ/4630067.html}
}

@article{liuBenchPrimeAccurateBenchmark2018,
  title = {{{BenchPrime}}: {{Accurate Benchmark Subsetting}} with {{Optimized Clustering Algorithm Selection}}},
  shorttitle = {{{BenchPrime}}},
  author = {Liu, Qingrui and Wu, Xiaolong and Kittinger, Larry and Levy, Markus and Jung, Changhee},
  year = {2018},
  month = aug,
  publisher = {{Department of Computer Science, Virginia Polytechnic Institute \& State University}},
  abstract = {This paper presents BenchPrime, an automated benchmark analysis toolset that is systematic and extensible to analyze the similarity and diversity of benchmark suites. BenchPrime takes multiple benchmark suites and their evaluation metrics as inputs and generates a hybrid benchmark suite comprising only essential applications. Unlike prior work, BenchPrime uses linear discriminant analysis rather than principal component analysis, as well as selects the best clustering algorithm and the optimized number of clusters in an automated and metric-tailored way, thereby achieving high accuracy. In addition, BenchPrime ranks the benchmark suites in terms of their application set diversity and estimates how unique each benchmark suite is compared to other suites. As a case study, this work for the first time compares the DenBench with the MediaBench and MiBench using four different metrics to provide a multi-dimensional understanding of the benchmark suites. For each metric, BenchPrime measures to what degree DenBench applications are irreplaceable with those in MediaBench and MiBench. This provides means for identifying an essential subset from the three benchmark suites without compromising the application balance of the full set. The experimental results show that the necessity of including DenBench applications varies across the target metrics and that significant redundancy exists among the three benchmark suites.},
  copyright = {In Copyright},
  langid = {american},
  annotation = {Accepted: 2018-08-25T17:45:08Z},
  file = {/home/fabian/Zotero/storage/PGWIL5SZ/Liu et al_2018_BenchPrime.pdf;/home/fabian/Zotero/storage/ZCSTC2TR/84916.html}
}

@misc{LowRISCIbex2021,
  title = {{{lowRISC}}/Ibex},
  year = {2021},
  month = jul,
  abstract = {Ibex is a small 32 bit RISC-V CPU core, previously known as zero-riscy.},
  copyright = {Apache-2.0},
  howpublished = {lowRISC},
  keywords = {cpucore,hardware,risc-v,rv32}
}

@article{machFPnewOpenSourceMultiformat2021,
  title = {{{FPnew}}: {{An Open-Source Multiformat Floating-Point Unit Architecture}} for {{Energy-Proportional Transprecision Computing}}},
  shorttitle = {{{FPnew}}},
  author = {Mach, Stefan and Schuiki, Fabian and Zaruba, Florian and Benini, Luca},
  year = {2021},
  month = apr,
  journal = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  volume = {29},
  number = {4},
  pages = {774--787},
  issn = {1557-9999},
  doi = {10.1109/TVLSI.2020.3044752},
  abstract = {The slowdown of Moore's law and the power wall necessitates a shift toward finely tunable precision (a.k.a. transprecision) computing to reduce energy footprint. Hence, we need circuits capable of performing floating-point operations on a wide range of precisions with high energy proportionality. We present FPnew, a highly configurable open-source transprecision floating-point unit (TP-FPU), capable of supporting a wide range of standard and custom FP formats. To demonstrate the flexibility and efficiency of FPnew in general-purpose processor architectures, we extend the RISC-V ISA with operations on half-precision, bfloat16, and an 8-bit FP format, as well as SIMD vectors and multiformat operations. Integrated into a 32-bit RISC-V core, our TP-FPU can speedup the execution of mixed-precision applications by 1.67\texttimes{} with respect to an FP32 baseline, while maintaining end-to-end precision and reducing system energy by 37\%. We also integrate FPnew into a 64-bit RISC-V core, supporting five FP formats on scalars or 2, 4, or 8-way SIMD vectors. For this core, we measured the silicon manufactured in Globalfoundries 22FDX technology across a wide voltage range from 0.45 to 1.2 V. The unit achieves leading-edge measured energy efficiencies between 178 Gflop/sW (on FP64) and 2.95 Tflop/sW (on 8-bit mini-floats), and a performance between 3.2 and 25.3 Gflop/s.},
  keywords = {Computer architecture,Energyefficient,floating-point unit,Hardware,multiformat,Open source software,Optimization,Pipeline processing,RISC-V,Silicon,Standards,transprecision computing},
  file = {/home/fabian/Zotero/storage/45VHXUZL/Mach et al. - 2021 - FPnew An Open-Source Multiformat Floating-Point U.pdf;/home/fabian/Zotero/storage/G398QDGJ/9311229.html}
}

@article{macnameeEmergingOnshipDebugging2000,
  title = {Emerging On-Ship Debugging Techniques for Real-Time Embedded Systems},
  author = {MacNamee, C. and Heffernan, D.},
  year = {2000},
  month = dec,
  journal = {Computing \&amp; Control Engineering Journal},
  volume = {11},
  number = {6},
  pages = {295--303},
  publisher = {{IET Digital Library}},
  issn = {1741-0460},
  doi = {10.1049/cce:20000608},
  abstract = {The increased clock frequencies and higher integration levels of today\&apos;s high-performance embedded microcontrollers have led to the widespread incorporation of on-chip debugging logic into new microcontroller chip designs. The newly defined standard for embedded system debugging, the IEEE-ISTO Nexus 5001 Forum Standard for a Global Embedded Debug Interface, is introduced and is related to the test and debugging requirements of development engineers.},
  langid = {english},
  file = {/home/fabian/Zotero/storage/VIL2NKDN/MacNamee_Heffernan_2000_Emerging on-ship debugging techniques for real-time embedded systems.pdf;/home/fabian/Zotero/storage/CD39R2IH/cce_20000608.html}
}

@inproceedings{marastoniDeepLearningApproach2018,
  title = {A Deep Learning Approach to Program Similarity},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Machine Learning}} and {{Software Engineering}} in {{Symbiosis}}},
  author = {Marastoni, Niccol{\`o} and Giacobazzi, Roberto and Dalla Preda, Mila},
  year = {2018},
  month = sep,
  series = {{{MASES}} 2018},
  pages = {26--35},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3243127.3243131},
  abstract = {In this work we tackle the problem of binary code similarity by using deep learning applied to binary code visualization techniques. Our idea is to represent binaries as images and then to investigate whether it is possible to recognize similar binaries by applying deep learning algorithms for image classification. In particular, we apply the proposed deep learning framework to a dataset of binary code variants obtained through code obfuscation. These binary variants exhibit similar behaviours while being syntactically different. Our results show that the problem of binary code recognition is strictly separated from simple image recognition problems. Moreover, the analysis of the results of the experiments conducted in this work lead us to the identification of interesting research challenges. For example, in order to use image recognition approaches to recognize similar binary code samples it is important to further investigate how to build a suitable mapping from executables to images.},
  isbn = {978-1-4503-5972-6},
  keywords = {Code similarity,code visualization,deep-learning,obfuscation},
  file = {/home/fabian/Zotero/storage/CRZ3GYXP/Marastoni et al_2018_A deep learning approach to program similarity.pdf}
}

@inproceedings{mayerDebugSupportCalibration2005,
  title = {Debug Support, Calibration and Emulation for Multiple Processor and Powertrain Control {{SoCs}} [Automotive Applications]},
  booktitle = {Design, {{Automation}} and {{Test}} in {{Europe}}},
  author = {Mayer, A. and Siebert, H. and {McDonald-Maier}, K. D.},
  year = {2005},
  month = mar,
  pages = {148-152 Vol. 3},
  issn = {1558-1101},
  doi = {10.1109/DATE.2005.109},
  abstract = {The introduction of complex SoCs with multiple processor cores presents new development challenges, such that development support is now a decisive factor when choosing a system-on-chip (SoC). The presented development support strategy addresses the challenges using both architecture and technology approaches. The multi-core debug support (MCDS) architecture provides flexible triggering using cross triggers and a multiple core break and suspend switch. Temporal trace ordering is guaranteed down to cycle level by on-chip time stamping. The package sized-ICE (PSI) approach is a novel method of including trace buffers, overlay memories, processing resources and communication interfaces without changing device behavior. PSI requires no external emulation box, as the debug host interfaces directly with the SoC using a standard interface.},
  keywords = {automotive electronic systems,automotive electronics,Automotive engineering,calibration,Calibration,circuit emulation,communication interfaces,Control systems,cross triggers,cycle level temporal trace ordering,debug host interface,development support,development systems,Emulation,flexible triggering,Ice,in-circuit emulators,logic testing,MCDS,Mechanical power transmission,multiple core break/suspend switch,multiple core debug support,multiple processor cores,multiprocessing systems,on-chip debug support circuits,on-chip time stamping,overlay memories,package sized-ICE,powertrain control SoC,Process control,processing resources,Production systems,program debugging,Switches,system calibration,System-on-a-chip,system-on-chip,trace buffers},
  file = {/home/fabian/Zotero/storage/SNW3FSKR/Mayer et al_2005_Debug support, calibration and emulation for multiple processor and powertrain.pdf;/home/fabian/Zotero/storage/D3DZ5NPA/1395811.html}
}

@misc{MPC555User2000,
  title = {{{MPC}} 555 {{User}}'s {{Manual Section}} 21, {{Development Support}}},
  year = {2000},
  month = oct,
  publisher = {{Freescale Semiconductor}},
  langid = {english},
  file = {/home/fabian/Zotero/storage/LGHANBH3/MPC555UM.pdf}
}

@inproceedings{nairFuncGNNGraphNeural2020,
  title = {{{funcGNN}}: {{A Graph Neural Network Approach}} to {{Program Similarity}}},
  shorttitle = {{{funcGNN}}},
  booktitle = {Proceedings of the 14th {{ACM}} / {{IEEE International Symposium}} on {{Empirical Software Engineering}} and {{Measurement}} ({{ESEM}})},
  author = {Nair, Aravind and Roy, Avijit and Meinke, Karl},
  year = {2020},
  month = oct,
  series = {{{ESEM}} '20},
  pages = {1--11},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3382494.3410675},
  abstract = {Background: Program similarity is a fundamental concept, central to the solution of software engineering tasks such as software plagiarism, clone identification, code refactoring and code search. Accurate similarity estimation between programs requires an in-depth understanding of their structure, semantics and flow. A control flow graph (CFG), is a graphical representation of a program which captures its logical control flow and hence its semantics. A common approach is to estimate program similarity by analysing CFGs using graph similarity measures, e.g. graph edit distance (GED). However, graph edit distance is an NP-hard problem and computationally expensive, making the application of graph similarity techniques to complex software programs impractical. Aim: This study intends to examine the effectiveness of graph neural networks to estimate program similarity, by analysing the associated control flow graphs. Method: We introduce funcGNN1, which is a graph neural network trained on labeled CFG pairs to predict the GED between unseen program pairs by utilizing an effective embedding vector. To our knowledge, this is the first time graph neural networks have been applied on labeled CFGs for estimating the similarity between highlevel language programs. Results: We demonstrate the effectiveness of funcGNN to estimate the GED between programs and our experimental analysis demonstrates how it achieves a lower error rate (1.94 x10-3), with faster (23 times faster than the quickest traditional GED approximation method) and better scalability compared with state of the art methods. Conclusion: funcGNN posses the inductive learning ability to infer program structure and generalise to unseen programs. The graph embedding of a program proposed by our methodology could be applied to several related software engineering problems (such as code plagiarism and clone identification) thus opening multiple research directions.},
  isbn = {978-1-4503-7580-1},
  keywords = {Attention Mechanism,Control Flow Graph,Graph Edit Distance,Graph Embedding,Graph Neural Network,Graph Similarity,Machine Learning,ProgramSimilarity,Software Engineering},
  file = {/home/fabian/Zotero/storage/DW338SPM/Nair et al_2020_funcGNN.pdf}
}

@misc{NiosIIClassic2016,
  title = {Nios {{II Classic Processor Reference Guide Section}} 2, {{Prozessor Architecture}}},
  year = {2016},
  month = oct,
  publisher = {{Altera}},
  langid = {english},
  file = {/home/fabian/Zotero/storage/VB3C9ULU/Nios II Classic Processor Reference Guide.pdf}
}

@misc{OpenhwgroupCv32e40p2021,
  title = {Openhwgroup/Cv32e40p},
  year = {2021},
  month = jun,
  abstract = {CV32E40P is an in-order 4-stage RISC-V RV32IMFCXpulp CPU based on RI5CY from PULP-Platform},
  howpublished = {OpenHW Group},
  keywords = {riscv,riscv32imfc}
}

@article{pallisterBEEBSOpenBenchmarks2013,
  title = {{{BEEBS}}: {{Open Benchmarks}} for {{Energy Measurements}} on {{Embedded Platforms}}},
  shorttitle = {{{BEEBS}}},
  author = {Pallister, James and Hollis, Simon and Bennett, Jeremy},
  year = {2013},
  month = sep,
  journal = {arXiv:1308.5174 [cs]},
  eprint = {1308.5174},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents and justifies an open benchmark suite named BEEBS, targeted at evaluating the energy consumption of embedded processors. We explore the possible sources of energy consumption, then select individual benchmarks from contemporary suites to cover these areas. Version one of BEEBS is presented here and contains 10 benchmarks that cover a wide range of typical embedded applications. The benchmark suite is portable across diverse architectures and is freely available. The benchmark suite is extensively evaluated, and the properties of its constituent programs are analysed. Using real hardware platforms we show case examples which illustrate the difference in power dissipation between three processor architectures and their related ISAs. We observe significant differences in the average instruction dissipation between the architectures of 4.4x, specifically 170uW/MHz (ARM Cortex-M0), 65uW/MHz (Adapteva Epiphany) and 88uW/MHz (XMOS XS1-L1).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Performance},
  file = {/home/fabian/Zotero/storage/GQT38LCL/Pallister et al_2013_BEEBS.pdf;/home/fabian/Zotero/storage/LHNR7WXL/1308.html}
}

@inproceedings{pandaWaitDecadeDid2018,
  title = {Wait of a {{Decade}}: {{Did SPEC CPU}} 2017 {{Broaden}} the {{Performance Horizon}}?},
  shorttitle = {Wait of a {{Decade}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Panda, R. and Song, S. and Dean, J. and John, L. K.},
  year = {2018},
  month = feb,
  pages = {271--282},
  issn = {2378-203X},
  doi = {10.1109/HPCA.2018.00032},
  abstract = {The recently released SPEC CPU2017 benchmark suite has already started receiving a lot of attention from both industry and academic communities. However, due to the significantly high size and complexity of the benchmarks, simulating all the CPU2017 benchmarks for design trade-off evaluation is likely to become extremely difficult. Simulating a randomly selected subset, or a random input set, may result in misleading conclusions. This paper analyzes the SPEC CPU2017 benchmarks using performance counter based experimentation from seven commercial systems, and uses statistical techniques such as principal component analysis and clustering to identify similarities among benchmarks. Such analysis can reveal benchmark redundancies and identify subsets for researchers who cannot use all benchmarks in pre-silicon design trade-off evaluations. Many of the SPEC CPU2006 benchmarks have been replaced with larger and complex workloads in the SPEC CPU2017 suite. However, compared to CPU2006, it is unknown whether SPEC CPU2017 benchmarks have different performance demands or whether they stress machines differently. Additionally, to evaluate the balance of CPU2017 benchmarks, we analyze the performance characteristics of CPU2017 workloads and compare them with emerging database, graph analytics and electronic design automation (EDA) workloads. This paper provides the first detailed analysis of SPEC CPU2017 benchmark suite for the architecture community.},
  keywords = {benchmark redundancies,Benchmark Redundancy Analysis,benchmark testing,Benchmark testing,C++ languages,CPU2017 workloads,EDA,electronic design automation,graph analytics,Industries,Measurement,microprocessor chips,performance evaluation,Performance Evaluation,principal component analysis,Principal component analysis,Redundancy,SPEC CPU 2017 broaden,SPEC CPU2006 benchmarks,SPEC CPU2017,SPEC CPU2017 benchmark suite,Stress},
  file = {/home/fabian/Zotero/storage/IMKYFAYC/Panda et al_2018_Wait of a Decade.pdf;/home/fabian/Zotero/storage/73NW8BQQ/8327015.html}
}

@inproceedings{pandaWaitDecadeDid2018a,
  title = {Wait of a {{Decade}}: {{Did SPEC CPU}} 2017 {{Broaden}} the {{Performance Horizon}}?},
  shorttitle = {Wait of a {{Decade}}},
  booktitle = {2018 {{IEEE International Symposium}} on {{High Performance Computer Architecture}} ({{HPCA}})},
  author = {Panda, Reena and Song, Shuang and Dean, Joseph and John, Lizy K.},
  year = {2018},
  month = feb,
  pages = {271--282},
  issn = {2378-203X},
  doi = {10.1109/HPCA.2018.00032},
  abstract = {The recently released SPEC CPU2017 benchmark suite has already started receiving a lot of attention from both industry and academic communities. However, due to the significantly high size and complexity of the benchmarks, simulating all the CPU2017 benchmarks for design trade-off evaluation is likely to become extremely difficult. Simulating a randomly selected subset, or a random input set, may result in misleading conclusions. This paper analyzes the SPEC CPU2017 benchmarks using performance counter based experimentation from seven commercial systems, and uses statistical techniques such as principal component analysis and clustering to identify similarities among benchmarks. Such analysis can reveal benchmark redundancies and identify subsets for researchers who cannot use all benchmarks in pre-silicon design trade-off evaluations. Many of the SPEC CPU2006 benchmarks have been replaced with larger and complex workloads in the SPEC CPU2017 suite. However, compared to CPU2006, it is unknown whether SPEC CPU2017 benchmarks have different performance demands or whether they stress machines differently. Additionally, to evaluate the balance of CPU2017 benchmarks, we analyze the performance characteristics of CPU2017 workloads and compare them with emerging database, graph analytics and electronic design automation (EDA) workloads. This paper provides the first detailed analysis of SPEC CPU2017 benchmark suite for the architecture community.},
  keywords = {Benchmark Redundancy Analysis,Benchmark testing,C++ languages,Industries,Measurement,Performance Evaluation,Principal component analysis,Redundancy,SPEC CPU2017,Stress},
  file = {/home/fabian/Zotero/storage/XU9WDB3L/Panda et al_2018_Wait of a Decade.pdf;/home/fabian/Zotero/storage/TP7GCLV4/8327015.html}
}

@article{pattersonRecruitingLongOverdue2019,
  title = {Recruiting for the {{Long Overdue}} and  {{Deserved Demise}} of {{Dhrystone}}},
  author = {Patterson, David},
  year = {2019},
  pages = {35},
  langid = {english},
  file = {/home/fabian/Zotero/storage/QNUCSP84/Lockwood - 1977 - Recruiting for the Long Overdue and  Deserved Demi.pdf}
}

@inproceedings{phansalkarMeasuringProgramSimilarity2005,
  title = {Measuring {{Program Similarity}}: {{Experiments}} with {{SPEC CPU Benchmark Suites}}},
  shorttitle = {Measuring {{Program Similarity}}},
  booktitle = {{{IEEE International Symposium}} on {{Performance Analysis}} of {{Systems}} and {{Software}}, 2005. {{ISPASS}} 2005.},
  author = {Phansalkar, A. and Joshi, A. and Eeckhout, L. and John, L. K.},
  year = {2005},
  month = mar,
  pages = {10--20},
  doi = {10.1109/ISPASS.2005.1430555},
  abstract = {It is essential that a subset of benchmark programs used to evaluate an architectural enhancement, is well distributed within the target workload space rather than clustered in specific areas. Past efforts for identifying subsets have primarily relied on using microarchitecture-dependent metrics of program performance, such as cycles per instruction and cache miss-rate. The shortcoming of this technique is that the results could be biased by the idiosyncrasies of the chosen configurations. The objective of this paper is to present a methodology to measure similarity of programs based on their inherent microarchitecture-independent characteristics which will make the results applicable to any microarchitecture. We apply our methodology to the SPEC CPU2000 benchmark suite and demonstrate that a subset of 8 programs can be used to effectively represent the entire suite. We validate the usefulness of this subset by using it to estimate the average IPC and L1 data cache miss-rate of the entire suite. The average IPC of 8-way and 16-way issue superscalar processor configurations could be estimated with 3.9\% and 4.4\% error respectively. This methodology is applicable not only to find subsets from a benchmark suite, but also to identify programs for a benchmark suite from a list of potential candidates. Studying the four generations of SPEC CPU benchmark suites, we find that other than a dramatic increase in the dynamic instruction count and increasingly poor temporal data locality, the inherent program characteristics have more or less remained the same},
  keywords = {Accuracy,Area measurement,benchmark testing,branch prediction,cache storage,Character generation,Data analysis,data locality,ILP,instruction level parallelism,instruction sets,L1 data cache miss-rate,Microarchitecture,microarchitecture-independent characteristics,microprocessor chips,Microprocessors,parallel architectures,Performance analysis,Process design,program similarity measurement,software approach,Space exploration,SPEC CPU benchmark suite,superscalar processor,Time measurement},
  file = {/home/fabian/Zotero/storage/DU8ZZ39R/Phansalkar et al_2005_Measuring Program Similarity.pdf;/home/fabian/Zotero/storage/5B3KHIAD/1430555.html}
}

@misc{PulpplatformPulpino2021,
  title = {Pulp-Platform/Pulpino},
  year = {2021},
  month = jun,
  abstract = {An open-source microcontroller system based on RISC-V},
  howpublished = {pulp-platform}
}

@misc{PulpplatformPulpissimo2021,
  title = {Pulp-Platform/Pulpissimo},
  year = {2021},
  month = apr,
  abstract = {This is the top-level project for the PULPissimo Platform. It instantiates a PULPissimo open-source system with a PULP SoC domain, but no cluster.},
  copyright = {View license         ,                 View license},
  howpublished = {pulp-platform}
}

@misc{RISCVInstructionSet2022,
  title = {{{RISC-V Instruction Set Manual}}},
  year = {2022},
  month = feb,
  abstract = {RISC-V Instruction Set Manual},
  copyright = {CC-BY-4.0},
  howpublished = {RISC-V}
}

@misc{RISCVTechnicalSpecification,
  title = {{{RISC-V Technical Specification}}},
  journal = {RISC-V International},
  howpublished = {https://riscv.org/technical/specifications/},
  langid = {american},
  file = {/home/fabian/Zotero/storage/4C6DWGN2/specifications.html}
}

@book{rolstadasBenchmarkingTheoryPractice2013,
  title = {Benchmarking \textemdash{} {{Theory}} and {{Practice}}},
  author = {Rolstad{\aa}s, Asbj{\o}rn},
  year = {2013},
  month = jun,
  publisher = {{Springer}},
  abstract = {Lecturers and researchers in the areas of industrial engineering, quality management and business development, and middle and higher management in business or technology- oriented positions, will find this book invaluable.},
  isbn = {978-0-387-34847-6},
  langid = {english},
  keywords = {Business \& Economics / Management,Business \& Economics / Production \& Operations Management,Technology \& Engineering / Manufacturing,Technology \& Engineering / Mechanical}
}

@article{saavedraAnalysisBenchmarkCharacteristics1996,
  title = {Analysis of Benchmark Characteristics and Benchmark Performance Prediction},
  author = {Saavedra, Rafael H. and Smith, Alan J.},
  year = {1996},
  month = nov,
  journal = {ACM Transactions on Computer Systems},
  volume = {14},
  number = {4},
  pages = {344--384},
  issn = {0734-2071},
  doi = {10.1145/235543.235545},
  abstract = {Standard benchmarking provides to run-times for given programs on given machines, but fails to provide insight as to why those results were obtained (either in terms of machine or program characteristics) and fails to provide run-times for that program on some other machine, or some other programs on that machine. We have developed a machine-imdependent model of program execution to characterize both machine performance and program execution. By merging these machine and program characterizations, we can estimate execution time for arbitrary machine/program combinations. Our technique allows us to identify those operations, either on the machine or in the programs, which dominate the benchmark results. This information helps designers in improving the performance of future machines and users in tuning their applications to better utilize the performance of existing machines. Here we apply our methodology to characterize benchmarks and predict their execution times. We present extensive run-time statistics for a large set of benchmarks including the SPEC and Perfect Club suites. We show how these statistics can be used to identify important shortcoming in the programs. In addition, we give execution time estimates for a large sample of programs and machines and compare these against benchmark results. Finally, we develop a metric for program similarity that makes it possible to classify benchmarks with respect to a large set of characteristics.},
  keywords = {abstract machine performance model,benchmark analysis,execution time prediction,hybrid approach,microbenchmarking},
  file = {/home/fabian/Zotero/storage/DJE5QSWQ/Saavedra_Smith_1996_Analysis of benchmark characteristics and benchmark performance prediction.pdf}
}

@inproceedings{schiavoneQuentinUltraLowPowerPULPissimo2018,
  title = {Quentin: An {{Ultra-Low-Power PULPissimo SoC}} in 22nm {{FDX}}},
  shorttitle = {Quentin},
  booktitle = {2018 {{IEEE SOI-3D-Subthreshold Microelectronics Technology Unified Conference}} ({{S3S}})},
  author = {Schiavone, Pasquale Davide and Rossi, Davide and Pullini, Antonio and Di Mauro, Alfio and Conti, Francesco and Benini, Luca},
  year = {2018},
  month = oct,
  pages = {1--3},
  issn = {2573-5926},
  doi = {10.1109/S3S.2018.8640145},
  abstract = {The End-Nodes of the Internet of Things (IoT) require extreme energy efficiency coupled with wide power-performance operating range. Fully-depleted SOI (FD-SOI) is an attractive technology for ultra-low power and wide-range operation as it offers compelling options to tune power, performance, area (PPA) at design time as well as at run time. This paper describes Quentin: an MCU-class (32bit) open-source RISC-V SoC featuring an autonomous I/O subsystem optimized to deal with the wide variety of sensors available in IoT end-nodes, coupled with a processor optimized for near threshold computation and a heterogeneous (standard-cell and SRAM) memory architecture to better exploit the low-voltage capabilities of 22nm FDX technology. The system runs up to 2400 million equivalent RV32IMC instructions per second (MOPS) and achieves best power density of 6 {$\mu$}W/MHz, resulting into an energy efficiency of 433 MOPS/mW.},
  keywords = {Bandwidth,FD-SOI,Internet of Things,IoT,Low-power electronics,MCU,microcontroller,Microcontrollers,Multicore processing,near-Threshold Computing,Random access memory,RISC-V},
  file = {/home/fabian/Zotero/storage/LV8D7IY4/Schiavone et al_2018_Quentin.pdf;/home/fabian/Zotero/storage/SRIVYQB7/8640145.html}
}

@inproceedings{simUsingBenchmarkingAdvance2003,
  title = {Using Benchmarking to Advance Research: A Challenge to Software Engineering},
  shorttitle = {Using Benchmarking to Advance Research},
  booktitle = {25th {{International Conference}} on {{Software Engineering}}, 2003. {{Proceedings}}.},
  author = {Sim, S. E. and Easterbrook, S. and Holt, R. C.},
  year = {2003},
  month = may,
  pages = {74--83},
  issn = {0270-5257},
  doi = {10.1109/ICSE.2003.1201189},
  abstract = {Benchmarks have been used in computer science to compare the performance of computer systems, information retrieval algorithms, databases, and many other technologies. The creation and widespread use of a benchmark within a research area is frequently accompanied by rapid technical progress and community building. These observations have led us to formulate a theory of benchmarking within scientific disciplines. Based on this theory, we challenge software engineering research to become more scientific and cohesive by working as a community to define benchmarks. In support of this challenge, we present a case study of the reverse engineering community, where we have successfully used benchmarks to advance the state of research.},
  keywords = {benchmark testing,benchmarking,Books,Collaboration,community building,Computer languages,Computer science,computer system performance,Databases,Guidelines,Information retrieval,information retrieval algorithms,reverse engineering,Reverse engineering,reverse engineering community,Software algorithms,software engineering,Software engineering,software engineering research,technical progress},
  file = {/home/fabian/Zotero/storage/G2XEFRE4/Sim et al_2003_Using benchmarking to advance research.pdf;/home/fabian/Zotero/storage/4SZGG8WB/1201189.html}
}

@inproceedings{sorensonEvaluatingSyntheticTrace2002,
  title = {Evaluating Synthetic Trace Models Using Locality Surfaces},
  booktitle = {2002 {{IEEE International Workshop}} on {{Workload Characterization}}},
  author = {{Sorenson} and {Flanagan}},
  year = {2002},
  month = nov,
  pages = {23--33},
  doi = {10.1109/WWC.2002.1226491},
  abstract = {In this paper we analyze several synthetic trace generation models using locality surfaces. The locality surfaces let us discover what elements of the real trace were accurately modeled and what features were not. None of the models examined are very good at retaining the locality of the real trace. We can see from cache simulation results that if the locality surface does not accurately reflect the locality of the real workload, the cache performance statistics will not be accurate either.},
  keywords = {Cache memories,Virtual computers},
  file = {/home/fabian/Zotero/storage/LIP32C8U/Sorenson_Flanagan_2002_Evaluating synthetic trace models using locality surfaces.pdf;/home/fabian/Zotero/storage/KNEVFHWD/1226491.html}
}

@misc{SystemRequirementsCPU,
  title = {System {{Requirements}} - {{CPU}} 2017},
  howpublished = {https://www.spec.org/cpu2017/Docs/system-requirements.html},
  file = {/home/fabian/Zotero/storage/T84YGIWU/system-requirements.html}
}

@misc{TC1775TriCoreUser2002,
  title = {{{TC1775 TriCore User}}'s {{Manual System Units Section}} 20, {{On-Chip Debug Support}}},
  year = {2002},
  month = may,
  publisher = {{Infineon Technologies}},
  langid = {english},
  file = {/home/fabian/Zotero/storage/PZQ8XLPN/TC1775.pdf}
}

@article{uhligTracedrivenMemorySimulation1997,
  title = {Trace-Driven Memory Simulation: A Survey},
  shorttitle = {Trace-Driven Memory Simulation},
  author = {Uhlig, Richard A. and Mudge, Trevor N.},
  year = {1997},
  month = jun,
  journal = {ACM Computing Surveys},
  volume = {29},
  number = {2},
  pages = {128--170},
  issn = {0360-0300},
  doi = {10.1145/254180.254184},
  abstract = {As the gap between processor and memory speeds continues to widen, methods for evaluating memory system designs before they are implemented in hardware are becoming increasingly important. One such method, trace-driven memory simulation, has been the subject of intense interest among researchers and has, as a result, enjoyed rapid development and substantial improvements during the past decade. This article surveys and analyzes these developments by establishing criteria for evaluating trace-driven methods, and then applies these criteria to describe, categorize, and compare over 50 trace-driven simulation tools. We discuss the strengths and weaknesses of different approaches and show that no single method is best when all criteria, including accuracy, speed, memory, flexibility, portability, expense, and ease of use are considered. In a concluding section, we examine fundamental limitations to trace-driven simulation, and survey some recent developments in memory simulation that may overcome these bottlenecks.},
  keywords = {caches,memory management,memory simulation,TLBs,trace-driven simulation},
  file = {/home/fabian/Zotero/storage/YZ8MPDIB/Uhlig_Mudge_1997_Trace-driven memory simulation.pdf}
}

@article{vandierendonckManyBenchmarksStress,
  title = {Many {{Benchmarks Stress}} the {{Same Bottlenecks}}},
  author = {Vandierendonck, Hans and Bosschere, Koen De},
  pages = {9},
  abstract = {The performance of a microprocessor is determined by many factors, including the memory hierarchy, clock frequency, organization, etc. A different trade-off between these factors is made in each microprocessor design. The performance is determined on the one hand by the bottlenecks of the machine (e.g., memory accesses, mispredicted branches, etc.) and their associated penalties and on the other hand by the frequency by which the bottlenecks occur, which is largely a property of the executed program. It is therefore important that a benchmark suite stresses all major bottlenecks in a microprocessor. If not, the benchmark suite gives a skewed view on performance. A method is presented to determine the most important bottlenecks stressed by benchmarks by analyzing their execution times. This method is applied to the SPEC CPU2000 benchmarks and it is shown that these benchmarks stress only about 4 important bottlenecks.},
  langid = {english},
  keywords = {hardware approach},
  file = {/home/fabian/Zotero/storage/ZM2B4E5J/Vandierendonck and Bosschere - Many Benchmarks Stress the Same Bottlenecks.pdf}
}

@inproceedings{vanertveldeBenchmarkSynthesisArchitecture2010,
  title = {Benchmark Synthesis for Architecture and Compiler Exploration},
  booktitle = {{{IEEE International Symposium}} on {{Workload Characterization}} ({{IISWC}}'10)},
  author = {Van Ertvelde, Luk and Eeckhout, Lieven},
  year = {2010},
  month = dec,
  pages = {1--11},
  doi = {10.1109/IISWC.2010.5650208},
  abstract = {This paper presents a novel benchmark synthesis framework with three key features. First, it generates synthetic benchmarks in a high-level programming language (C in our case), in contrast to prior work in benchmark synthesis which generates synthetic benchmarks in assembly. Second, the synthetic benchmarks hide proprietary information from the original workloads they are built after. Hence, companies may want to distribute synthetic benchmark clones to third parties as proxies for their proprietary codes; third parties can then optimize the target system without having access to the original codes. Third, the synthetic benchmarks are shorter running than the original workloads they are modeled after, yet they are representative. In summary, the proposed framework generates small (thus quick to simulate) and representative benchmarks that can serve as proxies for other workloads without revealing proprietary information; and because the benchmarks are generated in a high-level programming language, they can be used to explore both the architecture and compiler spaces. The results obtained with our initial framework are promising. We demonstrate that we can generate synthetic proxy benchmarks for the MiBench benchmarks, and we show that they are representative across a range of machines with different instruction-set architectures, microarchitectures, and compilers and optimization levels, while being 30 times shorter running on average. We also verify using software plagiarism detection tools that the synthetic benchmark clones hide proprietary information from the original workloads.},
  keywords = {Benchmark testing,Cloning,Computer architecture,Computer languages,Hardware,Optimization,Program processors},
  file = {/home/fabian/Zotero/storage/2N6CIVWW/Van Ertvelde_Eeckhout_2010_Benchmark synthesis for architecture and compiler exploration.pdf}
}

@inproceedings{vlaovicTAXITraceAnalysis2002,
  title = {{{TAXI}}: {{Trace Analysis}} for X86 {{Interpretation}}},
  shorttitle = {{{TAXI}}},
  booktitle = {Proceedings. {{IEEE International Conference}} on {{Computer Design}}: {{VLSI}} in {{Computers}} and {{Processors}}},
  author = {Vlaovic, S. and Davidson, E. S.},
  year = {2002},
  month = sep,
  pages = {508--514},
  issn = {1063-6404},
  doi = {10.1109/ICCD.2002.1106821},
  abstract = {Although x86 processors have been around for a long time and are the most ubiquitous processors in the world, the amount of academic research regarding details of their performance has been minimal. We introduce an x86 simulation environment, called TAXI (Trace Analysis for X86 Interpretation), and use it to present results for eight Win32 applications. In this paper, we explain the design and implementation of TAXI.},
  keywords = {Application software,Computational modeling,Computer architecture,Emulation,Hardware,Laboratories,Microarchitecture,performance evaluation,Software performance,Sun,TAXI,Timing,Trace Analysis for X86 Interpretation,virtual machines,Win32 applications,x86 processors,x86 simulation environment},
  file = {/home/fabian/Zotero/storage/TLJS3MAQ/Vlaovic and Davidson - 2002 - TAXI Trace Analysis for x86 Interpretation.pdf}
}

@article{vrecaAcceleratingDeepLearning2020,
  title = {Accelerating {{Deep Learning Inference}} in {{Constrained Embedded Devices Using Hardware Loops}} and a {{Dot Product Unit}}},
  author = {Vre{\v c}a, Jure and Sturm, Karl J. X. and Gungl, Ernest and Merchant, Farhad and Bientinesi, Paolo and Leupers, Rainer and Brezo{\v c}nik, Zmago},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {165913--165926},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3022824},
  abstract = {Deep learning algorithms have seen success in a wide variety of applications, such as machine translation, image and speech recognition, and self-driving cars. However, these algorithms have only recently gained a foothold in the embedded systems domain. Most embedded systems are based on cheap microcontrollers with limited memory capacity, and, thus, are typically seen as not capable of running deep learning algorithms. Nevertheless, we consider that advancements in compression of neural networks and neural network architecture, coupled with an optimized instruction set architecture, could make microcontroller-grade processors suitable for specific low-intensity deep learning applications. We propose a simple instruction set extension with two main components-hardware loops and dot product instructions. To evaluate the effectiveness of the extension, we developed optimized assembly functions for the fully connected and convolutional neural network layers. When using the extensions and the optimized assembly functions, we achieve an average clock cycle count decrease of 73\% for a small scale convolutional neural network. On a per layer base, our optimizations decrease the clock cycle count for fully connected layers and convolutional layers by 72\% and 78\%, respectively. The average energy consumption per inference decreases by 73\%. We have shown that adding just hardware loops and dot product instructions has a significant positive effect on processor efficiency in computing neural network functions.},
  keywords = {Biological neural networks,Deep learning,embedded systems,Embedded systems,Hardware,instruction set optimization,Instruction sets,Machine learning,Microcontrollers,RISC-V},
  file = {/home/fabian/Zotero/storage/RCJFMXPX/Vrea et al. - 2020 - Accelerating Deep Learning Inference in Constraine.pdf;/home/fabian/Zotero/storage/BBAZRYAL/9187807.html}
}

@inproceedings{williamsARMV8DebugTrace2012,
  title = {{{ARMV8}} Debug and Trace Architectures},
  booktitle = {Proceedings of the 2012 {{System}}, {{Software}}, {{SoC}} and {{Silicon Debug Conference}}},
  author = {Williams, M.},
  year = {2012},
  month = sep,
  pages = {1--6},
  issn = {2114-3684},
  abstract = {The ARM\textregistered{} processor family is the most widely used 32-bit processor family, and has consistently included debug features, starting from the ARM7TDMI\textregistered{} processor [1]. Embedded debug forms part of the ARMv7 architecture [2]. The most recent addition to the ARM architecture family is ARMv8 [3], which represents the biggest change in the architecture's history. This paper looks at the impact of the new architecture on debug and trace, and reviews some of the lessons learnt from previous architectures.},
  keywords = {ARM processor family,ARM7TDMI processor,ARMV8 debug,Complexity theory,computer architecture,Computer architecture,debug feature,embedded debug,Hardware,microprocessor chips,program debugging,program diagnostics,Protocols,Registers,Software,software debugging,System-on-a-chip,trace architecture,word length 32 bit},
  file = {/home/fabian/Zotero/storage/WXQ2A5NZ/Williams - 2012 - ARMV8 debug and trace architectures.pdf;/home/fabian/Zotero/storage/4YCART7U/6338158.html}
}

@inproceedings{yiStatisticallyRigorousApproach2003,
  title = {A Statistically Rigorous Approach for Improving Simulation Methodology},
  booktitle = {The {{Ninth International Symposium}} on {{High-Performance Computer Architecture}}, 2003. {{HPCA-9}} 2003. {{Proceedings}}.},
  author = {Yi, J. J. and Lilja, D. J. and Hawkins, D. M.},
  year = {2003},
  month = feb,
  pages = {281--291},
  issn = {1530-0897},
  doi = {10.1109/HPCA.2003.1183546},
  abstract = {Due to cost, time, and flexibility constraints, simulators are often used to explore the design space when developing new processor architectures, as well as when evaluating the performance of new processor enhancements. However, despite this dependence on simulators, statistically rigorous simulation methodologies are not typically used in computer architecture research. A formal methodology can provide a sound basis for drawing conclusions gathered from simulation results by adding statistical rigor, and consequently, can increase confidence in the simulation results. This paper demonstrates the application of a rigorous statistical technique to the setup and analysis phases of the simulation process. Specifically, we apply a Plackett and Burman design to: (1) identify key processor parameters; (2) classify benchmarks based on how they affect the processor; and (3) analyze the effect of processor performance enhancements. Our technique expands on previous work by applying a statistical method to improve the simulation methodology instead of applying a statistical model to estimate the performance of the processor.},
  keywords = {Analytical models,Application software,benchmark classification,Computational modeling,computer architecture,Computer architecture,Computer simulation,Costs,key processor parameters,microprocessor chips,Performance analysis,performance evaluation,Plackett Burman design,processor performance enhancements,simulation methodology,Space exploration,statistical analysis,Statistical analysis,statistically rigorous approach,Time factors,virtual machines},
  file = {/home/fabian/Zotero/storage/85UTNVCD/Yi et al_2003_A statistically rigorous approach for improving simulation methodology.pdf;/home/fabian/Zotero/storage/MJT93HQG/1183546.html}
}

@article{zhanBenchSubsetFrameworkSelecting,
  title = {{{BenchSubset}}: {{A}} Framework for Selecting Benchmark Subsets Based on Consensus Clustering},
  shorttitle = {{{BenchSubset}}},
  author = {Zhan, Hongping and Lin, Weiwei and Mao, Feiqiao and Xu, Minxian and Wu, Guangxin and Wu, Guokai and Li, Jianzhuo},
  journal = {International Journal of Intelligent Systems},
  volume = {n/a},
  number = {n/a},
  issn = {1098-111X},
  doi = {10.1002/int.22791},
  abstract = {The redundancy in the benchmark suite will increase the time for computer system performance evaluation and simulation. The most typical method to solve this problem is to select subsets based on clustering. However, it is a challenge to validate benchmark subsetting results for unlabeled benchmark suites when using the clustering method, and existing research has not considered this problem. Also, there is no quantitative evaluation method for subsetting which can reflect the universal and the diversity characteristics of the benchmark suite at the same time. To solve the above problems, we propose BenchSubset, a framework for selecting benchmark subsets based on consensus clustering, which includes Group Principal Components Analysis, consensus clustering, and a new evaluation method considering the universal and the diversity characteristics of the benchmark suite. We conducted SPEC CPU2017 subsetting experiments on Huawei's Taishan 200, then verified the effectiveness of BenchSubset in selecting a benchmark subset. Compared with the mainstream principal components analysis with hierarchical clustering (PCA-H) method, the benchmark subset selected by BenchSubset performs better in representing the universal and the diversity characteristics of SPEC CPU2017.},
  langid = {english},
  keywords = {benchmark subsets,consensus clustering,SPEC CPU2017},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.22791},
  file = {/home/fabian/Zotero/storage/CTXTYNSV/Zhan et al_BenchSubset.pdf;/home/fabian/Zotero/storage/TCZJAG5V/int.html}
}

@inproceedings{zhangUsingHardwareFeatures2015,
  title = {Using {{Hardware Features}} for {{Increased Debugging Transparency}}},
  booktitle = {2015 {{IEEE Symposium}} on {{Security}} and {{Privacy}}},
  author = {Zhang, F. and Leach, K. and Stavrou, A. and Wang, H. and Sun, K.},
  year = {2015},
  month = may,
  pages = {55--69},
  issn = {2375-1207},
  doi = {10.1109/SP.2015.11},
  abstract = {With the rapid proliferation of malware attacks on the Internet, understanding these malicious behaviors plays a critical role in crafting effective defense. Advanced malware analysis relies on virtualization or emulation technology to run samples in a confined environment, and to analyze malicious activities by instrumenting code execution. However, virtual machines and emulators inevitably create artifacts in the execution environment, making these approaches vulnerable to detection or subversion. In this paper, we present MALT, a debugging framework that employs System Management Mode, a CPU mode in the x86 architecture, to transparently study armored malware. MALT does not depend on virtualization or emulation and thus is immune to threats targeting such environments. Our approach reduces the attack surface at the software level, and advances state-of-the-art debugging transparency. MALT embodies various debugging functions, including register/memory accesses, breakpoints, and four stepping modes. We implemented a prototype of MALT on two physical machines, and we conducted experiments by testing an array of existing anti-virtualization, anti-emulation, and packing techniques against MALT. The experimental results show that our prototype remains transparent and undetected against the samples. Furthermore, our prototype of MALT introduces moderate but manageable overheads on both Windows and Linux platforms.},
  keywords = {Debugging,debugging transparency,emulation technology,Hardware,hardware features,Internet,invasive software,Kernel,Linux,Linux platforms,malicious behaviors,MALT,Malware,malware attacks,malware debugging,program debugging,rapid proliferation,Registers,Servers,SMM,software architecture,system management mode,transparency,virtual machines,virtualisation,Virtualization,virtualization technology,Windows platforms,x86 architecture},
  file = {/home/fabian/Zotero/storage/RDHCJBK6/Zhang et al. - 2015 - Using Hardware Features for Increased Debugging Tr.pdf;/home/fabian/Zotero/storage/BJ9LIERR/7163018.html}
}


